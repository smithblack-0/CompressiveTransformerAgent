{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16cb314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.10.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.21.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (56.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51bb14",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The majority of the content in this section is built to support the dynamic architecture. This is notable from a static architecture due to possessing an additional term, mean length, which is passed through each layer in addition to any primary tensors. It is the case that dynamic tensors will modify mean length, and furthermore that loss can be applied with respect to mean length\n",
    "\n",
    "## Architecture\n",
    "\n",
    "This project is built, suprisingly enough, in tensorflow. Each logical layer is expected to pass forward two parameters. These are the stream and mean length. Any modifications made to the mean length must be applied as such, and in a gradient-descent friendly format. This is, suprisingly enough, also possible. Finally, layers will exist which can modify the length of embeddings traversing the encoding. Most shockingly, this is, also, possible\n",
    "\n",
    "Notably, it is now also a requirment for higher dimensional tensors to work \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40367c40",
   "metadata": {},
   "source": [
    "## Base\n",
    "\n",
    "The base layer for the model. Contains a new space, \"forward\", to perform calculations in. Handles mean passthrough implicitly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603c459",
   "metadata": {},
   "source": [
    "# Basics\n",
    "\n",
    "Some of the basic tools I will need are defined here.\n",
    "\n",
    "* DynTensor - A utility to handle dynamic resized tensors and track needed info.\n",
    "* Base - The base layer\n",
    "* Local - A view of a tensor dimension in local notation.\n",
    "* View - A nice view function for modifying only the last few dimensions of a tenosr. \n",
    "* Linear - A tweaked linear layer. Capable of preserving heads\n",
    "* PosEncodings - Basic positional encodings.\n",
    "* Transformer - A tweaked transformer. Capable of generating, working with, and collapsing predefined heads when requested\n",
    "* Feedforward - A tweaked feedforward layer. Capable of preserving heads if requested.\n",
    "* SortPartition - Handle vectorized batched masks producing entries of inequivalent length. Pads, sorts, keeps longest neded length. Rest is zeros.\n",
    "* LengthEncode - Takes a string of entries, and their mean lengths. Forms from each of them \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb50ef5",
   "metadata": {},
   "source": [
    "## Tensor Dynamics\n",
    "\n",
    "Tensor Dynamics is a small microlibrary consisting of a set of tools utilized to greatly ease the management of tensors when performing both standard and dynamic operations. It is designed to greatly ease the workload of a tensor programmer by allowing named dimensions and dynamic corrolation. It has the following design philosophy:\n",
    "\n",
    "* Relevant Dimension Policy: One should only have to worry about the relevant dimensions when performing an operatin\n",
    "* Implicit Tracking: Anything which is not being explictly modified should stay the same.\n",
    "* Easy Dynamics: It should be easy to form dynamic masks and dynamic thresholds\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Tensor Dynamics consists of three primary classes. These are the DynSpec, the DynTensor, and the DynRestore. These work together to create tensor dynamics. Tensor dynamics consists of allowing the masking of particular dimensions, with respect to another dimension, in order to virtually shorten the length of a tensor. In particular, let\n",
    "\n",
    "**DynSpec**\n",
    "\n",
    "The DynSpec defines the dimensions of a tensor, along with any dynamic linkage which occurs. Configuration errors are not checked in this class - none\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "065673ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynSpec():\n",
    "    \"\"\"\n",
    "\n",
    "    A DynSpec. An editable container where spec information can be kept.\n",
    "    \n",
    "    Does not by itself error check. Does, however, contain methods\n",
    "    \"insert\", \"add\", \"replace\", \"remove\", and \"front\" allowing\n",
    "    the easy editing of what is inside of it. To construct a spec\n",
    "    this way, make sure to start it with DynSpec() - no parameters\n",
    "    \n",
    "    .spec contains the actual table. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _column = (\"labels\", \"pointers\", \"lengths\", \"param\")\n",
    "    def make_frame(self, label, pointer, length, param):\n",
    "        \"\"\"\n",
    "        Makes a dataframe which can be appended or inserted as needed.\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        label_data = self._column\n",
    "        raw_data = [[label], [pointer], [length], [param]]\n",
    "        build_data = dict(zip(label_data, raw_data))\n",
    "        return pd.DataFrame(build_data)\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, labels, pointers=None, lengths=None, params=None):\n",
    "        \"\"\"\n",
    "        The create method. Something of an alternative to the __init__ method, create will\n",
    "        take a series of lists meant to represent a spec and go create a spec out of the list.\n",
    "        \n",
    "        It will then return the DynSpec.\n",
    "        \n",
    "        For each dimension:\n",
    "            label must be a string\n",
    "            pointer and length must be None or (string, 1D_tensor)\n",
    "            length, if 1d, must equal tensor length.            \n",
    "        Across Dimensions:\n",
    "            pointer must point towards another existing label.  \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Handle implicit nones\n",
    "        \n",
    "        if pointers is None:\n",
    "            pointer = [None]*len(labels)\n",
    "        if lengths is None:\n",
    "            lengths = [None]*len(labels)\n",
    "        if params is None:\n",
    "            params = [None]*len(labels)\n",
    "        \n",
    "        #zip entries together, make construction dictionaries.\n",
    "        \n",
    "        raw_data = [labels, pointers, lengths, params]\n",
    "        raw_labels = cls._column\n",
    "        data = dict(zip(raw_labels, raw_data))\n",
    "        \n",
    "        #Create pandas frame, make DynSpec\n",
    "        \n",
    "        spec = pd.DataFrame(data)\n",
    "        return DynSpec(spec)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __init__(self, spec=None):\n",
    "        \"\"\"\n",
    "        Startup. Store a spec if available, otherwise start a generic table.\n",
    "        \"\"\"\n",
    "        if spec is None:\n",
    "            self.spec = pd.DataFrame(columns= self._column)\n",
    "        else:\n",
    "            self.spec = spec\n",
    "    \n",
    "    def front(self, label, pointer=None, length=None, param=None):\n",
    "        \n",
    "        #Create frame.\n",
    "        frame = self.make_frame(label, pointer, length, param)\n",
    "        \n",
    "        #Store it at the front of the spec\n",
    "        spec = pd.concat([frame, self.spec], axis=0, ignore_index=True)\n",
    "        self.spec = spec\n",
    "    \n",
    "    def insert(self, index, label, pointer=None, length=None, param=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        Insert a dimension somewhere inside the spec.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #Create frame\n",
    "        frame = self.make_frame(label, pointer, length, param)\n",
    "        \n",
    "        #slice apart spec\n",
    "        \n",
    "        prior = self.spec.iloc[:index, :]\n",
    "        post = self.spec.iloc[index:, :]\n",
    "        \n",
    "        #create new spec\n",
    "        spec = pd.concat([prior, frame, post], ignore_index=True)\n",
    "        self.spec = spec        \n",
    "        \n",
    "    def append(self, label, pointer=None, length=None, param=None):\n",
    "        \"\"\"\n",
    "        Append a new dimension to the end of the spec.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #create frame\n",
    "        frame = self.make_frame(label, pointer, length, param)\n",
    "    \n",
    "        #create spec\n",
    "        spec = pd.concat([self.spec, frame], axis=0, ignore_index =True)\n",
    "        \n",
    "        #create return\n",
    "        \n",
    "        return DynSpec(spec)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1601b40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testAppend (__main__.TestDynSpec) ... ok\n",
      "testCreate (__main__.TestDynSpec) ... ok\n",
      "testFront (__main__.TestDynSpec) ... ok\n",
      "testInsert (__main__.TestDynSpec) ... ok\n",
      "testIsSane (__main__.TestDynTensor) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: testIsSane (__main__.TestDynTensor)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_21104/393677500.py\", line 52, in testIsSane\n",
      "    NotTensor()\n",
      "  File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_21104/393677500.py\", line 25, in NotTensor\n",
      "    self.assertTrue(self.is_code(DynTensor.is_sane(None, None), 0))\n",
      "AttributeError: type object 'DynTensor' has no attribute 'is_sane'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.288s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1b10dc3a7f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class TestDynSpec(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.spec = DynSpec()\n",
    "    def testFront(self):\n",
    "        self.spec.front(\"test\")\n",
    "        self.spec.front(\"test2\", \"test\", tf.constant([3]))\n",
    "        self.spec.front(\"test3\", param = \"blink\")\n",
    "    def testInsert(self):\n",
    "        self.spec.insert(0, \"test4\")\n",
    "        self.spec.insert(0, \"test5\", param=\"tweak\")\n",
    "        self.spec.insert(1, \"test6\")\n",
    "    def testAppend(self):\n",
    "        self.spec.append(\"test\")\n",
    "        self.spec.append(\"test2\")\n",
    "        self.spec.append(\"test3\")\n",
    "    def testCreate(self):\n",
    "        labels = [\"a\", \"b\", \"c\"]\n",
    "        param = [None, \"test\", None]\n",
    "        \n",
    "        item1 = self.spec.create(labels)\n",
    "        item2 = self.spec.create(labels, params = param)\n",
    "                \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2c128c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile\n",
      "compile\n",
      "tf.Tensor([b'a' b'b' b'c'], shape=(3,), dtype=string)\n",
      "compile\n",
      "tf.Tensor([b'd' b'e' b'f'], shape=(3,), dtype=string)\n",
      "recompile?\n",
      "tf.Tensor([b'a' b'b' b'c'], shape=(3,), dtype=string)\n",
      "tf.Tensor([b'a' b'b' b'c'], shape=(3,), dtype=string)\n",
      "tf.Tensor([b'a' b'b' b'c'], shape=(3,), dtype=string)\n",
      "tf.Tensor([b'a' b'b' b'c'], shape=(3,), dtype=string)\n",
      "compile\n",
      "tf.Tensor([b'a' b'b' b'c'], shape=(3,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "class SpecError(Exception):\n",
    "    def __init__(self, msg, code, index):\n",
    "        self.code = code\n",
    "        super().__init__(msg)\n",
    "\n",
    "\n",
    "class DynTensor():\n",
    "    \"\"\"\n",
    "    \n",
    "    The DynTensor consists of the management engine for dynamics. A DynTensor is started\n",
    "    with a spec, and a tensor. The spec must be a valid DynSpec, and the tensor must \n",
    "    match the spec. Upon being started, the DynTensor will boot up it's utility functions,\n",
    "    set up it's mask function, and standby for further orders. \n",
    "    \n",
    "    Among other things, it can be ordered to mask, by the rules of dynamics, \n",
    "    the current tensor, \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def is_sane(tensor, spec):\n",
    "        \"\"\"\n",
    "        \n",
    "        The primary sanity checking method. This returns true if sane, \n",
    "        and the reason not sane if false\n",
    "        \n",
    "                \n",
    "        For each dimension:\n",
    "            tensor: \n",
    "                should be tensor\n",
    "            spec:\n",
    "                per dimension:\n",
    "                    every entry in label must be a string\n",
    "                    pointer and length must be None or (string, 1D_tensor)\n",
    "                Across Dimensions:\n",
    "                    pointer must point towards another existing label. \n",
    "            interdependent:\n",
    "                spec length should equal tensor length\n",
    "                length, if 1d, must equal tensor length.            \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #Tensor independent checks\n",
    "        \n",
    "        if not tf.is_tensor(tensor):\n",
    "            return SpecError(\"Tensor was not actually a tensor\", 0, None)\n",
    "        \n",
    "        \n",
    "\n",
    "        #Perform base length checks.\n",
    "        if len(spec.spec[\"labels\"]) != len(tensor.shape):\n",
    "            return SpecError(\"Tensor and Spec are of different lengths\", 1, (spec.spec, tensor.shape))\n",
    "        #Create primary iteration frame\n",
    "        \n",
    "        \n",
    "        primary_iteration = pd.concat([spec.spec, pd.DataFrame({\"shape\" : tensor.shape})], axis=1)\n",
    "        #Perform dimensional self-consistency check. The majority of the \n",
    "        #handling exists here.\n",
    "        pointers = []\n",
    "        for index, (label, pointer, length, name, shape) in primary_iteration.iterrows():\n",
    "            #Check label property\n",
    "            if type(label) != str:\n",
    "                return SpecError(\"Label at index %s was not a string\" % index, 2, index)\n",
    "            \n",
    "            #Check pointer, length types\n",
    "            if not (type(pointer) == str or pointer is None):\n",
    "                return SpecError(\"Pointer at label %s was neither string nor None\" % label, 3, index)\n",
    "            if not (tf.is_tensor(length) or length is None):\n",
    "                return SpecError(\"Length at label %s was neither tensor nor None\" % label, 4, index)\n",
    "            if (pointer is not None) and (pointer not in primary_iteration[\"labels\"].array):\n",
    "                return SpecError(\"Pointer at label %s points to label %s, which does not exist\" % (label, pointer), 5, index)\n",
    "        \n",
    "            #Check pointer, length compatibility\n",
    "            if type(pointer) == str and not tf.is_tensor(length):\n",
    "                return SpecError(\"Pointer at label %s was string, but length was not tensor\" % label, 6, index)\n",
    "            if pointer is None and length is not None:\n",
    "                return SpecError(\"Pointer at label %s was None, but length was not None\" % label, 6, index)\n",
    "            \n",
    "            #Check length details, and compatibity\n",
    "            if length is not None:\n",
    "                if len(length.shape) != 1:\n",
    "                    return SpecError(\"Length at label %s was not a 1D tensor. It was %s d\" % (label, len(length.shape)), 7,  index)\n",
    "                if length.shape[0] != shape:\n",
    "                    return SpecError(\"Length at label %s did not match tensor dimension. Got %s, but tensor is %s\" % (label, length.shape[0], shape), 7, index) \n",
    "\n",
    "        #Return true\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def __init__(self, tensor, spec, skip_error_check=False):\n",
    "        \"\"\"\n",
    "        The initialization method for the DynTensor\n",
    "        \n",
    "        The DynTensor must be fed a DynSpec, which will in turn be utilized to \n",
    "        initialize the tensor. \n",
    "             \n",
    "        \n",
    "        \"\"\"\n",
    "        #Handle sanity checking. See is_sane for details, but after this point the spec and tensor are known\n",
    "        #to be compatible.\n",
    "        if self.is_sane(tensor, spec) is not True and skip_error_check is not True:\n",
    "            raise self.is_sane(tensor, spec)\n",
    "        \n",
    "        #Create instance spec. \n",
    "        \n",
    "        primary_spec = pd.concat([spec.spec, pd.DataFrame({\"shape\" : tensor.shape})], axis=1)\n",
    "        self.spec = primary_spec\n",
    "        #Create focused pandas arrays\n",
    "        \n",
    "        dyn_spec = primary_spec[primary_spec[\"pointers\"] != None]\n",
    "        target_spec = primary_spec[primary_spec[\"labels\"].isin(dyn_spec[\"pointers\"])]\n",
    "        \n",
    "        #Create needed iteration arrays. Then perform iteration\n",
    "        \n",
    "        \n",
    "        #dyn_lengths = dyn_spec[\"lengths\"]        \n",
    "        #target_shape = target_spec[\"shape\"]\n",
    "        #mask_frame = pd.concat([dyn_lengths, target_shape], axis=1)\n",
    "        \n",
    "        \n",
    "        #Iterate. Generate masks. Apply. \n",
    "        #for index, (length, shape) in mask_frame.iterrows():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0f541b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abs__', '__abstractmethods__', '__add__', '__and__', '__bool__', '__class__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__tf_tracing_type__', '__truediv__', '__weakref__', '__xor__', '_abc_impl', '_as_graph_element', '_consumers', '_convert_values_and_partition', '_eager_value', '_from_nested_row_partitions', '_from_row_partition', '_from_variant', '_is_eager', '_nested_row_partitions', '_row_partition', '_set_shape', '_shape_invariant_to_type_spec', '_tf_api_names', '_tf_api_names_v1', '_to_variant', '_type_spec', '_values', 'bounding_shape', 'consumers', 'dtype', 'flat_values', 'from_nested_row_lengths', 'from_nested_row_splits', 'from_nested_value_rowids', 'from_row_lengths', 'from_row_limits', 'from_row_splits', 'from_row_starts', 'from_sparse', 'from_tensor', 'from_uniform_row_length', 'from_value_rowids', 'get_shape', 'merge_dims', 'nested_row_lengths', 'nested_row_splits', 'nested_value_rowids', 'nrows', 'numpy', 'ragged_rank', 'row_lengths', 'row_limits', 'row_splits', 'row_starts', 'shape', 'to_list', 'to_sparse', 'to_tensor', 'uniform_row_length', 'value_rowids', 'values', 'with_flat_values', 'with_row_splits_dtype', 'with_values']\n",
      "<bound method RaggedTensor.nested_row_lengths of <tf.RaggedTensor [[[1.0, 2.0, 3.0]],\n",
      " [[4.0, 5.0, 6.0, 7.0]]]>>\n",
      "2\n",
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n",
      "<tf.RaggedTensor [[[1.0, 2.0, 3.0]],\n",
      " [[4.0, 5.0, 6.0, 7.0]]]>\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "Tensor(\"index:0\", shape=(4,), dtype=float32)\n",
      "<tf.RaggedTensor [[0.0, 0.0, 0.0], [0.0, 0.0], [0.0],\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensor_list = [tf.constant([[1., 2., 3.]]), tf.constant([[4., 5., 6., 7.]])]\n",
    "\n",
    "ragged = tf.ragged.stack(tensor_list)\n",
    "\n",
    "print(dir(ragged))\n",
    "print(ragged.nested_row_lengths)\n",
    "print(ragged.ragged_rank)\n",
    "print(ragged[0][0])\n",
    "print(tf.map_fn(lambda x : x, ragged))\n",
    "print(ragged.bounding_shape(axis=2))\n",
    "\n",
    "testa = tf.constant([3.4, 2.7, 1.2, 9.1])\n",
    "@tf.function\n",
    "def make_length_parallel(index):\n",
    "    print(index)\n",
    "    spec = tf.RaggedTensorSpec([None])\n",
    "    output = tf.map_fn(fn=lambda x : tf.cast(tf.zeros([tf.cast(x, tf.int32)]), dtype=tf.float32), elems=index,\n",
    "                      fn_output_signature=spec)\n",
    "    return output\n",
    "\n",
    "print(make_length_parallel(testa))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a368f6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testAppend (__main__.TestDynSpec) ... ok\n",
      "testCreate (__main__.TestDynSpec) ... ok\n",
      "testFront (__main__.TestDynSpec) ... ok\n",
      "testInsert (__main__.TestDynSpec) ... ok\n",
      "testIsSane (__main__.TestDynTensor) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.308s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1b10dca2940>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestDynTensor(unittest.TestCase):\n",
    "\n",
    "    \n",
    "    #Test sanity checking function. Fairly complex.\n",
    "    \n",
    "    #Basic helper functions\n",
    "    def is_code(self, result, code):\n",
    "        self.assertTrue(isinstance(result, SpecError), \"Did not recieve error when required\")\n",
    "        return result.code == code\n",
    "    def is_not_code(self, result, code):\n",
    "        if isinstance(result, SpecError):\n",
    "            if result.code == code:\n",
    "                return False\n",
    "        return True\n",
    "    #Code by code helper functions.\n",
    "    \n",
    "    \n",
    "    def testIsSane(self):\n",
    "        \n",
    "        #Create testing functions.\n",
    "        \n",
    "        def NotTensor():\n",
    "            tensor = tf.ones([10, 3, 4])\n",
    "            spec = DynSpec.create([\"a\", \"b\", \"c\"])\n",
    "            self.assertTrue(self.is_code(DynTensor.is_sane(None, None), 0))\n",
    "            self.assertTrue(self.is_not_code(tensor, spec), 0)\n",
    "        def NotSameLength():\n",
    "            code = 1\n",
    "            \n",
    "            tensor = tf.ones([10, 4, 3])\n",
    "            spec_pass = DynSpec.create([\"a\", \"b\", \"c\"])\n",
    "            spec_fail = DynSpec.create([\"a\", \"b\"])\n",
    "            \n",
    "            test_pass = DynTensor.is_sane(tensor, spec_pass)\n",
    "            test_fail = DynTensor.is_sane(tensor, spec_fail)\n",
    "            \n",
    "            self.assertTrue(self.is_not_code(test_pass, code), \"Same length passed when it should have failed\")\n",
    "            self.assertTrue(self.is_code(test_fail, code), \"Same length failed when it should have passed\")\n",
    "        def LabelsAreString():\n",
    "            code = 2\n",
    "            \n",
    "            tensor = tf.ones([10, 4, 3])\n",
    "            spec_pass = DynSpec.create([\"a\", \"b\", \"c\"])\n",
    "            spec_fail = DynSpec.create([\"a\", None, \"c\"])\n",
    "        \n",
    "            test_pass = DynTensor.is_sane(tensor, spec_pass)\n",
    "            test_fail = DynTensor.is_sane(tensor, spec_fail)\n",
    "            \n",
    "            self.assertTrue(self.is_not_code(test_pass, code), \"List of string labels failed when it shoudl pass\")\n",
    "            self.assertTrue(self.is_code(test_fail, code), \"Labels test passed when it should of failed\")\n",
    "        def BadPointerType():\n",
    "            code = 3\n",
    "            \n",
    "            tensor = tf.ones([10])\n",
    "            spec_pass = DynSpec.create([\"a\"], [\"b\"], [None])\n",
    "            spec_pass2 = DynSpec.create([\"a\"], None, None)\n",
    "            spec_fail = DynSpec.create([\"a\"], [[]])\n",
    "            \n",
    "            test_pass1 = DynTensor.is_sane(tensor, spec_pass)\n",
    "            test_pass2 = DynTensor.is_sane(tensor, spec_pass2)\n",
    "            test_fail = DynTensor.is_sane(tensor, spec_fail)\n",
    "            \n",
    "            self.assertTrue(self.is_not_code(test_pass1, code), \"Pointer type failure\")\n",
    "            self.assertTrue(self.is_not_code(test_pass2, code), \"Pointer type failure\")\n",
    "            self.assertTrue(self.is_code(test_fail, code), \"Pointer type failure\")\n",
    "        def BadLengthType():\n",
    "            code = 4\n",
    "            \n",
    "            tensor = tf.ones([10])\n",
    "            spec_pass_1 = DynSpec.create([\"a\"], [\"b\"], [tf.constant([3])])\n",
    "            spec_pass_2 = DynSpec.create([\"a\"], [\"b\"], [None])\n",
    "            spec_fail = DynSpec.create([\"a\"], [\"b\"], [3])\n",
    "            \n",
    "            test_pass1 = DynTensor.is_sane(tensor, spec_pass_1)\n",
    "            test_pass2 = DynTensor.is_sane(tensor, spec_pass_2)\n",
    "            test_fail = DynTensor.is_sane(tensor, spec_fail)\n",
    "            \n",
    "            self.assertTrue(self.is_not_code(test_pass1, code), \"LengthType issue\")\n",
    "            self.assertTrue(self.is_not_code(test_pass2, code), \"LengthType issue\")\n",
    "            self.assertTrue(self.is_code(test_fail, code), \"LengthType Issue\")\n",
    "        def BadPointing():\n",
    "            code = 5\n",
    "            \n",
    "            tensor = tf.ones([10, 3])\n",
    "            spec_pass = DynSpec.create([\"a\", \"b\"], [\"b\", None], [tf.constant([10]), None])\n",
    "            spec_fail = DynSpec.create([\"a\", \"b\"], [\"c\", None], [None, None])\n",
    "            \n",
    "            \n",
    "            test_pass = DynTensor.is_sane(tensor, spec_pass)\n",
    "            test_fail = DynTensor.is_sane(tensor, spec_fail)\n",
    "            \n",
    "            \n",
    "            self.assertTrue(self.is_not_code(test_pass, code), \"Pointing exists failed\")\n",
    "            self.assertTrue(self.is_code(test_fail, code), \"Pointing exists failed.\")\n",
    "        def BadCorrolated():\n",
    "            code = 6\n",
    "            \n",
    "            tensor = tf.ones([10, 3])\n",
    "            spec_pass = DynSpec.create([\"a\", \"b\"], [\"b\", None], [tf.constant([10]), None])\n",
    "            spec_faila = DynSpec.create([\"a\", \"b\"], [\"b\", None], [None, None])\n",
    "            spec_failb =  DynSpec.create([\"a\", \"b\"], [None, None], [tf.constant([10]), None])\n",
    "            \n",
    "            test_pass = DynTensor.is_sane(tensor, spec_pass)\n",
    "            test_faila = DynTensor.is_sane(tensor, spec_faila)\n",
    "            test_failb = DynTensor.is_sane(tensor, spec_failb)\n",
    "                        \n",
    "            self.assertTrue(self.is_not_code(test_pass, code), \"Corrolation failure\")\n",
    "            self.assertTrue(self.is_code(test_faila, code), \"Corrolation failure\")\n",
    "            self.assertTrue(self.is_code(test_failb, code), \"Corrolation failure\")\n",
    "        def BadLength():\n",
    "            code = 7\n",
    "            \n",
    "            tensor = tf.ones([10, 3])\n",
    "            spec_pass = DynSpec.create([\"a\", \"b\"], [\"b\", None], [tf.ones([10]), None])\n",
    "            spec_faila =  DynSpec.create([\"a\", \"b\"], [\"b\", None], [tf.ones([8]), None])\n",
    "            spec_failb = DynSpec.create([\"a\", \"b\"], [\"b\", None], [tf.constant(8), None])\n",
    "            \n",
    "            test_pass = DynTensor.is_sane(tensor, spec_pass)\n",
    "            test_faila = DynTensor.is_sane(tensor, spec_faila)\n",
    "            test_failb = DynTensor.is_sane(tensor, spec_failb)\n",
    "                        \n",
    "            self.assertTrue(self.is_not_code(test_pass, code), \"Length Shape failure\")\n",
    "            self.assertTrue(self.is_code(test_faila, code), \"Length Shape  failure\")\n",
    "            self.assertTrue(self.is_code(test_failb, code), \"Length Shape failure\")\n",
    "            \n",
    "        #Execute the tests.\n",
    "    \n",
    "        NotTensor()\n",
    "        NotSameLength()\n",
    "        LabelsAreString()\n",
    "        BadPointerType()\n",
    "        BadLengthType()\n",
    "        BadPointing()\n",
    "        BadCorrolated()\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb1f7562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = pd.DataFrame({\"a\": [1, 2,3], \"b\" : [4,  5, 6]})\n",
    "c = pd.DataFrame({\"c\" : [6, 7, 8]})\n",
    "\n",
    "pd.concat([frame, c], axis=1)\n",
    "[None]*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8db206ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynTensor():\n",
    "    \"\"\"\n",
    "    The DynTensor consists of the means\n",
    "    and methods needed in order to manage\n",
    "    a dynamic instance. This includes\n",
    "    logic to manage keyed dimension\n",
    "    manipulation along with dynamic \n",
    "    masking.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #define properties.\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "    @property\n",
    "    def tensor(self):\n",
    "        return self._tensor\n",
    "    @property\n",
    "    def dyn(self):\n",
    "        return self._dyn\n",
    "    #Define startup function.\n",
    "    @staticmethod\n",
    "    def create(tensor, labels):\n",
    "        \"\"\"\n",
    "        \n",
    "        Creates a new DynTensor with dynamic stream from an ordinary tensorflow\n",
    "        tensor. \n",
    "        \n",
    "        Additional dimensions starting with \"d_\" are created for each original\n",
    "        dimension, and a dynlist is filled with None's to match. \n",
    "        \"\"\"\n",
    "        \n",
    "        #For each dimension, create a dynamic mirror\n",
    "        final_tensor = tensor\n",
    "        for dim in tensor.shape:\n",
    "            final_tensor = tf.expand_dims(final_tensor, axis=0) \n",
    "        \n",
    "        #For each label, make a dynamic mirror\n",
    "        \n",
    "        dynamic = [\"d_\" + item for item in labels]\n",
    "        final_labels = dynamic + labels\n",
    "        \n",
    "        #For each dyn, make the current length\n",
    "        \n",
    "        dynamic_dyn = [tf.constant([dim], dtype=tf.float32) for dim in tensor.shape]\n",
    "        static_dyn = [None]*len(tensor.shape)\n",
    "        final_dyn = dynamic_dyn + static_dyn\n",
    "        \n",
    "        #Create and return the DynTensor. Do not screw this up.\n",
    "        \n",
    "        return DynTensor(final_tensor, final_labels, final_dyn)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Define primary functions\n",
    "    def compress(self, final_labels):\n",
    "        \"\"\"\n",
    "        \n",
    "        This is a core logical method\n",
    "        \n",
    "        The compress method will reorder the dimensions of the input tensors to be in the order\n",
    "        given in labels. It will do this for the tensor and the dynlist. It will then compress\n",
    "        away the non-accessed dimensions into a single dimension at the front of the tensor, \n",
    "        and return four items: \n",
    "        \n",
    "        tensor, labels, dyn, restore\n",
    "        \n",
    "        Tensor indicates the compressed tensor, and may be directly manipulated. labels are the labels\n",
    "        which are actually in use. dyn contains any dynamic elements. restore, when run with \n",
    "        a tensor label pair, or tensor-label-dyn pair, will restore the compressed elements.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #Reorder in the proper sequence. \n",
    "        tensor, labels, dyn = self._reorder(final_labels)\n",
    "        \n",
    "        #Separate shape info, labels info, dyn info into two piles. One goes into restoration.\n",
    "        #The other will be used for reshape and return.\n",
    "        \n",
    "        slice_length = len(final_labels)\n",
    "        \n",
    "        implicit_dims, explicit_dims = tensor.shape[:-slice_length], tensor.shape[-slice_length:]\n",
    "        implicit_labels, explicit_labels = labels[:-slice_length], labels[-slice_length:]\n",
    "        implicit_dyn, explicit_dyn  = dyn[:-slice_length], dyn[-slice_length:]\n",
    "        \n",
    "        #Use the implicit portions to form a restore class. Use the explicit portions to reshape the tensor\n",
    "        \n",
    "        shape = [-1, *explicit_dims]\n",
    "        final_tensor = tf.reshape(tensor, shape)\n",
    "        \n",
    "        restore = DynRestore(implicit_dims, implicit_labels, implicit_dyn)\n",
    "        \n",
    "        #Return the compressed properties\n",
    "        return final_tensor, explicit_labels, explicit_dyn, restore\n",
    "        \n",
    "    def mask(self, fill=0):\n",
    "        \"\"\"\n",
    "        This has the effect of performing a filled mask of the internal quantities. \n",
    "        \n",
    "        The Dyn quantity of the DynTensor is read, and should be a float. If the float is less than the current dimension,\n",
    "        the portion is included. \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Create index pairs. These are pairs of dimensional indices which are known to be corrolated.\n",
    "        \n",
    "        lookup_index = dict(zip(self.labels, range(len(self.labels))))\n",
    "        dynamics = [item for item in self.labels if item.startswith(\"d_\")]\n",
    "        statics = [item[2:] for item in dynamics]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "        #Create construction mesh. This consists of, per dimension, an indication\n",
    "        #of how far above the zero index the current dimension lies.\n",
    "        \n",
    "        grid_make = [tf.cast(tf.range(dim), tf.float32) for dim in self.tensor.shape]\n",
    "        grid = tf.meshgrid(*grid_make)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Create mask operator. Do this by taking the grid, considering each dimension, and putting together\n",
    "        #a 1, 0 mask that can be multiplied.\n",
    "        intermediary = []\n",
    "        for grid_item, dyn_item in zip(grid, self.dyn):\n",
    "            if dyn_item is not None:\n",
    "                result = tf.where(grid_item <= dyn_item, 1., 0.)\n",
    "                intermediary.append(result)\n",
    "        intermediary = tf.stack(intermediary, axis=0)\n",
    "        mask = tf.reduce_prod(intermediary, axis=0)\n",
    "        \n",
    "        #Apply mask effect. \n",
    "        \n",
    "        tensor = self._tensor * mask\n",
    "        \n",
    "        #Construct new DynTensor. Return it\n",
    "        \n",
    "        return DynTensor(tensor, self.labels, self.dyn)\n",
    "        \n",
    " \n",
    "    def _reorder(self, labels):\n",
    "        \"\"\"\n",
    "        \n",
    "        A core function, this reorders the tensor, names, and dynlist to be in a \n",
    "        particular order, implicitly retaining order on the rest of the dimensions. \n",
    "        \n",
    "        It then returns the result of these reorders\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Figure out what the new, reordered labels will look like.\n",
    "        \n",
    "        implicit_labels = []\n",
    "        for item in self.labels:\n",
    "            if item not in labels:\n",
    "                implicit_labels.append(item)\n",
    "        \n",
    "        final_labels = implicit_labels + labels\n",
    "        \n",
    "        #Create transformation lookup tables. These will go ahead and tells us what index needs to end up\n",
    "        #where, or be used directly for transformations.\n",
    "        \n",
    "        lookup_indices = dict(zip(self.labels, range(len(self.labels))))\n",
    "        lookup_dyn = dict(zip(self.labels, self.dyn))\n",
    "        \n",
    "        #Create permuter. This is used by tensorflow to rearrange the dimensions.\n",
    "        \n",
    "        permute = [lookup_indices[item] for item in final_labels]\n",
    "        \n",
    "        #Perform final transforms.\n",
    "        \n",
    "        final_tensor = tf.transpose(self.tensor, permute)\n",
    "        final_labels = final_labels\n",
    "        final_dyn = [lookup_dyn[item] for item in final_labels]\n",
    "        \n",
    "      \n",
    "        #Return results.\n",
    "        return final_tensor, final_labels, final_dyn\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __init__(self, tensor, dim_labels, dyn):\n",
    "        \n",
    "        \n",
    "        #Perform sanity checking. Dyn entries must be width of dimensions or None, and \n",
    "        #tensor dims, dim_labels, and dyn must be the same length.\n",
    "        \n",
    "        assert len(tensor.shape) == len(dim_labels), \"Label length and tensor dimensons do not match\"\n",
    "        assert len(dim_labels) == len(dyn), \"Label length and dyn length do not match.\"\n",
    "\n",
    "        for dim, item_dyn, label in zip(tensor.shape, dyn, dim_labels):\n",
    "            if item_dyn is not None:\n",
    "                assert tf.is_tensor(item_dyn), \"Item in dyn at position %s was not None or tf Tensor\" % label\n",
    "                assert len(item_dyn.shape) == 1, \"Item %s in dyn was not 1d\" % label\n",
    "                assert item_dyn.shape[0] == dim, \"Dyn Tensor %s was length %s while tensor dim was %s\" % (label, item_dyn.shape[0], dim)\n",
    "                assert item_dyn.dtype == tf.float32, \"Dyn value was not equal to float32\"\n",
    "        #Storage\n",
    "        self._tensor = tensor\n",
    "        self._labels = dim_labels\n",
    "        self._dyn = dyn    \n",
    "        \n",
    "    \n",
    "\n",
    "class DynRestore():\n",
    "    \"\"\"\n",
    "    The DynRestore class is designed to restore collapsed dimensions.\n",
    "    \n",
    "    When provided with just a tensor with labels, it will use the final dimension,\n",
    "    which is expected to be unlabeled, to attempt to re-expand the problem back into\n",
    "    it's original format. It will also reconstruct the dynlist to match the incoming tensors. \n",
    "    \n",
    "    When provided with a DynList as well, it will use that to assign new dynamic masks, rather than \n",
    "    an implicit fill. If \n",
    "    \"\"\"\n",
    "    def __init__(self, implicit_dims, implicit_labels, implicit_dyn):\n",
    "        \n",
    "        self._implicit_dims = implicit_dims\n",
    "        self._implicit_labels = implicit_labels\n",
    "        self._implicit_dyn = implicit_dyn\n",
    "        \n",
    "    def __call__(self, tensor, labels, dyn=None):\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Construct the DynList if not given\n",
    "        \n",
    "        if dyn is None:\n",
    "            dyn = [None]*len(labels)\n",
    "            \n",
    "            \n",
    "        #Concatenate together various lists and quantities\n",
    "        \n",
    "        final_dims = [*self._implicit_dims, *tensor.shape[1:]]\n",
    "        final_labels = [*self._implicit_labels, *labels]\n",
    "        final_dyn = [*self._implicit_dyn, *dyn]\n",
    "        \n",
    "        \n",
    "        #Create a shape for reshape. Execute. Then create DynTensor and return\n",
    "        \n",
    "        shape = final_dims\n",
    "        final_tensor = tf.reshape(tensor, shape)\n",
    "        \n",
    "        return DynTensor(final_tensor, final_labels, final_dyn)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a59fab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testCompression (__main__.TestDynamic)\n",
      "Test a compression instance ... ok\n",
      "testDirectRestore (__main__.TestDynamic)\n",
      "Tests direct restore. Do I get back out what I put in? ... ok\n",
      "testMask (__main__.TestDynamic)\n",
      "Test whether the mask function is working sanely ... C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_8428/3295938291.py:101: DeprecationWarning: Please use assertEqual instead.\n",
      "  self.assertEquals(tf.reduce_sum(masked_tensor.tensor), 600)\n",
      "FAIL\n",
      "testResizeRestore (__main__.TestDynamic)\n",
      "Test resize restore. If I fiddle with the provided dimensions, can I still restore the others? ... ok\n",
      "testSetup (__main__.TestDynamic)\n",
      "See if the set up values are sane ... ok\n",
      "test_isupper (__main__.TestStringMethods) ... ok\n",
      "test_split (__main__.TestStringMethods) ... ok\n",
      "test_upper (__main__.TestStringMethods) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([10.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([4.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([5.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([3.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "tf.Tensor(1200.0, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>, None, None, None, None, None, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>]\n",
      "(1, 1, 1, 1, 10, 4, 5, 3, 2, 1)\n",
      "tf.Tensor([4.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([5.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([3.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([2.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([5.], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: testMask (__main__.TestDynamic)\n",
      "Test whether the mask function is working sanely\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_8428/3295938291.py\", line 101, in testMask\n",
      "    self.assertEquals(tf.reduce_sum(masked_tensor.tensor), 600)\n",
      "AssertionError: <tf.Tensor: shape=(), dtype=float32, numpy=1200.0> != 600\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.419s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1e038b19280>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test \n",
    "class TestDynamic(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        \"\"\"\n",
    "        Create source dynamic tensor for testing. Also, see if basic setup works\n",
    "        \n",
    "        \"\"\"\n",
    "        tensor = tf.ones([10, 4, 5, 3,2])\n",
    "        label = [\"batch\", \"row\", \"column\", \"item\", \"channel\"]\n",
    "        self._dyn_tensor = DynTensor.create(tensor, label)\n",
    "        self._labels = label\n",
    "    def testSetup(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        See if the set up values are sane\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        dyn_tensor = self._dyn_tensor\n",
    "        label = [\"d_batch\", \"d_row\", \"d_column\", \"d_item\", \"d_channel\"] + self._labels\n",
    "        \n",
    "        self.assertEqual(dyn_tensor.tensor.shape, [1, 1, 1, 1, 1, 10, 4, 5, 3, 2]) #one additional for each dynamic\n",
    "        self.assertEqual(dyn_tensor.labels, label) #Includes dynamic channels\n",
    "        \n",
    "        for label_item, dyn_item in zip(dyn_tensor.labels, dyn_tensor.dyn):\n",
    "            if dyn_item is not None:\n",
    "                self.assertTrue(tf.is_tensor(dyn_item))\n",
    "                self.assertTrue(dyn_item.dtype == tf.float32)\n",
    "        \n",
    "    def testCompression(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        Test a compression instance\n",
    "        \n",
    "        \"\"\"\n",
    "        dyn_tensor = self._dyn_tensor\n",
    "        selection = [\"d_batch\", \"batch\", \"row\", \"channel\"]\n",
    "        tensor, labels, dyn, restore = dyn_tensor.compress(selection)\n",
    "        \n",
    "        self.assertEqual(tensor.shape[0], 15, \"Final shape improper\")\n",
    "        self.assertEqual(labels, selection, \"Returned labels and selection did not match\")\n",
    "    def testDirectRestore(self):\n",
    "        \"\"\"\n",
    "        Tests direct restore. Do I get back out what I put in? \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Primary logic\n",
    "        dyn_tensor = self._dyn_tensor\n",
    "        selection = [\"item\",\"channel\"]\n",
    "        tensor, labels, dyn, restore = dyn_tensor.compress(selection)\n",
    "        restore_tensor = restore(tensor, labels, dyn)\n",
    "        \n",
    "        #Tests\n",
    "        self.assertEqual(dyn_tensor.tensor.shape, restore_tensor.tensor.shape, \"Tensor shapes inequal\")\n",
    "        self.assertEqual(dyn_tensor.labels, restore_tensor.labels, \"Labels inequal\")\n",
    "        self.assertEqual(dyn_tensor.dyn, restore_tensor.dyn, \"Dyn inequal\")\n",
    "    def testResizeRestore(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        Test resize restore. If I fiddle with the provided dimensions, can I still restore the others?\n",
    "        \n",
    "        \"\"\"\n",
    "        dyn_tensor = self._dyn_tensor\n",
    "        selection = [\"d_batch\", \"item\", \"row\"]\n",
    "        tensor, labels, dyn, restore = dyn_tensor.compress(selection)\n",
    "        \n",
    "        #Expand.\n",
    "        \n",
    "        expand_tensor = tf.expand_dims(tensor, axis=-1)\n",
    "        expand_tensor = tf.repeat(expand_tensor, 3, axis=-1)\n",
    "        labels.append(\"local\")\n",
    "        dyn.append(None)\n",
    "        \n",
    "        #Try \n",
    "\n",
    "        restore(expand_tensor, labels)\n",
    "        restore(expand_tensor, labels, dyn)\n",
    "    def testMask(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        Test whether the mask function is working sanely\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        dyn_tensor = self._dyn_tensor\n",
    "        \n",
    "        #Static test. No masking\n",
    "        masked_tensor = dyn_tensor.mask() \n",
    "        self.assertTrue(tf.reduce_all(dyn_tensor.tensor == masked_tensor.tensor))\n",
    "        \n",
    "        #Setup mask. Check effectiveness\n",
    "        \n",
    "        tensor, labels, dyn, restore = dyn_tensor.compress([\"d_batch\"])\n",
    "        print(tf.reduce_sum(dyn_tensor.tensor))\n",
    "        dyn[0] = tf.constant([5.])\n",
    "        masked_tensor = restore(tensor, labels, dyn)\n",
    "        print(masked_tensor.dyn)\n",
    "        print(masked_tensor.tensor.shape)\n",
    "        masked_tensor = masked_tensor.mask()\n",
    "        self.assertEquals(tf.reduce_sum(masked_tensor.tensor), 600)\n",
    "        \n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8ee61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "874e8219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties check\n",
      "['d_batch', 'd_rows', 'd_columns', 'd_items', 'd_channels', 'batch', 'rows', 'columns', 'items', 'channels']\n",
      "(1, 1, 1, 1, 1, 10, 3, 4, 5, 2)\n",
      "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([10.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>, None, None, None, None, None]\n",
      "\n",
      "compress check\n",
      "(240, 1, 5)\n",
      "['d_batch', 'items']\n",
      "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([10.], dtype=float32)>, None]\n",
      "<__main__.DynRestore object at 0x000001E0004DC2E0>\n",
      "direct restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DynTensor at 0x1e03d9c94f0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e2d718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch', 'row', 'column', 'item', 'channel']\n",
      "['batch', 'item', 'channel', 'row', 'column']\n",
      "(800, 5, 4, 1)\n",
      "['batch', 'item', 'channel', 'row', 'column', 'local']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SymTensor():\n",
    "    \"\"\"\n",
    "    A class with a variety of sanity-easing utility functions.\n",
    "    \n",
    "    The SymTensor class provides a key-dim corrolation between tensors and their dimensions - \n",
    "    effectively allowing the naming and manipulation of dimensions. It's capabilities are modest, but sufficient.\n",
    "    It allows one to request a reordering of the dimensions in a particular order, a packing of tensor\n",
    "    dimensions in a reduction, performing updates, and more.\n",
    "    \n",
    "    \"\"\"\n",
    "    @property\n",
    "    def tensor(self):\n",
    "        return self._tensor\n",
    "    @property\n",
    "    def names(self):\n",
    "        return self._names\n",
    "    @property\n",
    "    def lengths(self):\n",
    "        return self._lengths\n",
    "    def __init__(self, tensor, dim_names):\n",
    "        \"\"\"\n",
    "        The initialization routine. This should store both the tensor, and the dimension\n",
    "        names. It should be the case that the number of tensor dimensions, and dim_names, is the same\n",
    "        \"\"\"\n",
    "        assert len(tensor.shape) == len(dim_names)\n",
    "        \n",
    "        self._tensor = tensor\n",
    "        self._names = dim_names\n",
    "        self._lengths = tensor.shape\n",
    "        \n",
    "    def compress(self, selection):\n",
    "        \"\"\"\n",
    "        \n",
    "        Keeps around, in the designated order, everything in selection.\n",
    "        Everything else gets compressed into one last dimension. Then\n",
    "        returns the tensor, and a names index.\n",
    "        \"\"\"\n",
    "        \n",
    "        tensor, names, length = self._reorder(selection)\n",
    "        reshape = [-1, *tensor.shape[-len(selection):]]\n",
    "        \n",
    "        names = names[-len(selection):]\n",
    "        tensor = tf.reshape(tensor, reshape)\n",
    "        \n",
    "        return tensor, names\n",
    "    def like(self, tensor, definition_names):\n",
    "        \"\"\"\n",
    "        Attempts to take in a tensor, and make a SymTensor \"like\" it using the information \n",
    "        in names. \n",
    "        \n",
    "        Anything provided in names should corrolate to tensor dimensions. These will not be inferred,\n",
    "        but handled directly. Any dimension which is in tensor, but not the input names, will be inferred to \n",
    "        be reshape dimensions. These dimensions will be reshaped to match the inferred dimensions of the current\n",
    "        instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Get names, lengths from current instance. Cut out regions of inference.\n",
    "        region_width = len(definition_names)\n",
    "        \n",
    "        #Get the inferred names and lengths. These are the names and lengths that are NOT\n",
    "        #defined by the incoming definition names.\n",
    "        inferred_names, inferred_lengths = [], []\n",
    "        for name, length in zip(self.names, self.lengths):\n",
    "            if name not in definition_names:\n",
    "                inferred_names.append(name)\n",
    "                inferred_lengths.append(length)\n",
    "        \n",
    "        #Get the direct names  and direct lengths. These are defined directly.\n",
    "        \n",
    "        direct_names, direct_lengths = definition_names, [*tensor.shape[-region_width:]]\n",
    "        \n",
    "        #Create names shape, reshape. Return.\n",
    "        new_names = inferred_names + direct_names\n",
    "        new_lengths = inferred_lengths + direct_lengths\n",
    "        new_tensor = tf.reshape(tensor, new_lengths)\n",
    "        \n",
    "        return SymTensor(new_tensor, new_names)\n",
    "    def reorder(self, selection):\n",
    "        \"\"\"\n",
    "        \n",
    "        Reorders the tensor to be in a particular order.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        tensor, names, length = self._reorder(selection)\n",
    "        return SymTensor(tensor, names)\n",
    "        \n",
    "    \n",
    "    def _reorder(self, selection):\n",
    "        \"\"\"\n",
    "        \n",
    "        A core function. Allows the implicit reordering of the tensor and associated quantities.\n",
    "        \n",
    "        Names given are brought to the end of the tensor, the rest are shuffled near the beginning in the current order.\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        #Create permuter. This will rearrange both the tensor, and the list. Do this by looping through names\n",
    "        #and shuffling them into two catagories - end, which remains ordered as declaration, and start, which is implicit\n",
    "        \n",
    "        names = []\n",
    "        for item in self.names:\n",
    "            if item not in selection: #Not being shifted to the end\n",
    "                names.append(item)\n",
    "                \n",
    "        names = names + selection\n",
    "        \n",
    "        #Convert the permuter to a permute, and use to develop the correct output\n",
    "        \n",
    "        lookup = dict(zip(self.names, range(len(self.names))))\n",
    "        permute = [lookup[name] for name in names]\n",
    "        tensor = tf.identity(tf.transpose(self.tensor, permute))\n",
    "        \n",
    "        #Return results\n",
    "        \n",
    "        return tensor, names, tensor.shape\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "test_tensor = tf.zeros([10, 5,  4, 20, 4])\n",
    "test_names = [\"batch\", \"row\", \"column\", \"item\", \"channel\"]\n",
    "dyn_test = SymTensor(test_tensor, test_names)\n",
    "print(dyn_test.names)\n",
    "dyn_test2 = dyn_test.reorder([\"row\", \"column\"])\n",
    "print(dyn_test2.names)\n",
    "dyn_test3, names = dyn_test2.compress(['row', 'column'])\n",
    "dyn_test3 = tf.expand_dims(dyn_test3, -1)\n",
    "print(dyn_test3.shape)\n",
    "dyn_test4 = dyn_test2.like(dyn_test3, [\"row\", \"column\", \"local\"])\n",
    "print(dyn_test4.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ea79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "37a8e697",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18428/1011455403.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "test = list(range(5))\n",
    "index = [0,2]\n",
    "print(test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bcaf61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaggedOpLib():\n",
    "    \"\"\"\n",
    "    \n",
    "    There are several operations, principly in the domain of ragged tensors, \n",
    "    which I lack the ability to utilize dynamically. \n",
    "    \n",
    "    This is an attempt to get around some of that\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    to_tensor = staticmethod(tf.RaggedTensor.to_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "edcb8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Base(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    \n",
    "    The base layer. This layer will happily unstack\n",
    "    a ragged tensor, with ragged dimensions at the front, and forward the result to \n",
    "    the method called \"forward.\" It is, naturally, up to the implimentation\n",
    "    to decide what to do with it from there. \n",
    "    \n",
    "    One thing to note, however, is that when making shape-dependent logic, one should\n",
    "    use tf.shape, NOT tensor.shape. The latter will only be queried once, while the\n",
    "    former will build itself into the tensorflow graph.\n",
    "    \n",
    "    Both increasing, and decreasing, the depth of the ragged stack are also responsibilities\n",
    "    the layer assists with. One can define, on initialization, a delta_depth, or d_depth, which\n",
    "    influences how unstacking and restacking occurs. A positive d_depth will attempt to increase\n",
    "    the depth by the indicated d_depth. For a negative d_depth, meanwhile, it is vice versa. Both cases will \n",
    "    expect changes to the forward method\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_depth=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._d_depth = d_depth #positive - make deeper. Negative - make shallower.\n",
    " \n",
    "    def preprocessing(self, tensor, )\n",
    "    def forward(self, tensor, parameters, *args):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        raise NotImplimentedError(\"Forward must be implimented.\")\n",
    "    @tf.function\n",
    "    def _broken_call(self, tensor, scratchspace, *args):\n",
    "        \"\"\"\n",
    "        Does not work. Cannot compile until I have a better tf.shape function.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "               \n",
    "        print(\"compile\")\n",
    "        print(tensor.shape[0])\n",
    "        \n",
    "        #Recursion over. Breakout. \n",
    "        #Note that the ==() checks for the case of a depth=0 or raggedless tensor.\n",
    "        #Note that negative d_depth stops recursion early.\n",
    "        if scratchspace.shape == () or scratchspace.shape[0] == max(-self._d_depth, 0):\n",
    "            #Recursion depth reached\n",
    "            if not tf.is_tensor(tensor):\n",
    "                #Not a tensor, so convert to one.\n",
    "                tensor = tensor.to_tensor()\n",
    "            \n",
    "            return self.forward(tensor, scratchspace, *args)\n",
    "            \n",
    "              \n",
    "        #Initiate and perform recursion. Remember to slice off pieces to account for descent.\n",
    "        scratch_bypass, scratch_shrank = scratchspace[0], scratchspace[1:]\n",
    "        tensor_output, scratch_output = [], []     \n",
    "        for index in tf.range(int(tensor.shape[0])):\n",
    "            \n",
    "            item = tensor[index]\n",
    "            bypass = scratch_bypass[index]\n",
    "            \n",
    "            output = self._call(item, bypass)\n",
    "            result, scratch = self._broken_call(item, bypass)\n",
    "\n",
    "            \n",
    "            tensor_output.append(result)\n",
    "            if scratch is None:\n",
    "                #Just add the bypass to the stack\n",
    "                scratch_output.append(bypass)\n",
    "            else:\n",
    "                #place the scratch tensor, which should be 2d, on the end of the bypass value,\n",
    "                #which is 0d.\n",
    "                scratch_output.append(tf.concat([[bypass], scratch], axis=0))\n",
    "        \n",
    "        #Stack results\n",
    "        print(tensor_output)\n",
    "        tensor_output = tf.ragged.stack(tensor_output, axis=0)\n",
    "        scratch_output = tf.concat(scratch_output, axis=0)\n",
    "        \n",
    "        #return\n",
    "        \n",
    "        return tensor_output, scratch_output\n",
    "                \n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def _call(self, tensor, depth, parameters, *args):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #Terminate recursion logic.Note the max processing - this stops us early if needed\n",
    "        #for tensorflow processing\n",
    "        if depth.shape == () or depth.shape[0] == tf.max(-self._d_depth, 0):\n",
    "            #Recursion depth reached\n",
    "            if not tf.is_tensor(tensor):\n",
    "                #Not a tensor, so convert to one.\n",
    "                tensor = tensor.to_tensor()\n",
    "            \n",
    "            return self.forward(tensor, depth, parameters, *args)\n",
    "        \n",
    "        \n",
    "        #Not ending recursion. Proceed further into chain. Squeeze another dimension off of\n",
    "        #scratchspace, , \n",
    "    \n",
    "        shape = tensor.shape[:]\n",
    "        shape = [*shape[1:]] #One dimension will be unpacked\n",
    "\n",
    "        if self._d_depth > 0:\n",
    "            shape = [*shape, *[None]*self._d_depth] #One additional dimension for each increase.\n",
    "        spec = tf.RaggedTensorSpec(shape, dtype=tf.float32)\n",
    "        \n",
    "        func = lambda tensor : self._call(tensor, depth[1:], parameters)\n",
    "        output = tf.map_fn(func, tensor, fn_output_signature=spec)\n",
    "        \n",
    "        return output\n",
    "        #shape = tensor.shape\n",
    "        #shape = [2, *shape[1:]] #One more dimension, to account for scratch carry dimension. Remove dimension\n",
    "        #spec = tf.RaggedTensorSpec(shape, dtype=tf.float32) #Remove a \n",
    "\n",
    "        #func = lambda tensor : self._call(tensor, scratchspace, *args) #note the depth slice. Reducing count\n",
    "        #scratch_out, tensor_out = tf.map_fn(func, tensor, fn_output_signature=spec) \n",
    "        \n",
    "        #if isinstance(scratch_out, tf.RaggedTensor):\n",
    "            #scratch_out = scratch_out.to_tensor()\n",
    "        \n",
    "        #Unsqueeze scratchspacek, \n",
    "        #while len(scratch_out.shape) > 2:\n",
    "            #scratch_out = tf.squeeze(scratch_out, axis=0)\n",
    "                \n",
    "        \n",
    "        #return output\n",
    "    \n",
    "    def call(self, x, *args):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #Unpack depth, tensor. Depth is a 1D tensor, for which the value of shape[0] determines\n",
    "        #the number of unpack steps to perform. This was needed to allow for compile.\n",
    "        #\n",
    "        #Yea, tensorflow is weird.\n",
    "        \n",
    "        if tf.is_tensor(x): #Handle startup.\n",
    "            x = [x, tf.ragged.stack([[]])]\n",
    "        \n",
    "        #Unpack. Perform primary processing.\n",
    "        tensor, depth, parameters = x\n",
    "        output = self._call(tensor, depth, parameters, *args)\n",
    "        \n",
    "        if \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "634bd881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0],\n",
      "  [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]],\n",
      " [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]]>\n",
      "<tf.RaggedTensor [[0.0, 0.0]]>\n",
      "compile\n",
      "2\n",
      "compile\n",
      "None\n",
      "[tf.RaggedTensor(values=Tensor(\"while/PartitionedCall_1:0\", shape=(None,), dtype=float32), row_splits=Tensor(\"while/PartitionedCall_1:1\", shape=(None,), dtype=int64))]\n"
     ]
    },
    {
     "ename": "InaccessibleTensorError",
     "evalue": "Exception encountered when calling layer \"test_17\" (type Test).\n\nin user code:\n\n    File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\", line 82, in _broken_call  *\n        tensor_output = tf.ragged.stack(tensor_output, axis=0)\n\n    InaccessibleTensorError: <tf.Tensor 'while/PartitionedCall_1:0' shape=(None,) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'while/PartitionedCall_1:0' shape=(None,) dtype=float32> was defined here:\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n          return _run_code(code, main_globals, None,\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\runpy.py\", line 87, in _run_code\n          exec(code, run_globals)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n          app.launch_new_instance()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n          app.start()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 667, in start\n          self.io_loop.start()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n          self.asyncio_loop.run_forever()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n          self._run_once()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n          handle._run()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\events.py\", line 80, in _run\n          self._context.run(self._callback, *self._args)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 456, in dispatch_queue\n          await self.process_one()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 445, in process_one\n          await dispatch(*args)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 352, in dispatch_shell\n          await result\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 647, in execute_request\n          reply_content = await reply_content\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 345, in do_execute\n          res = shell.run_cell(code, store_history=store_history, silent=silent)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n          return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2898, in run_cell\n          result = self._run_cell(\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2944, in _run_cell\n          return runner(coro)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n          coro.send(None)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3169, in run_cell_async\n          has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_ast_nodes\n          if (await self.run_code(code, result,  async_=asy)):\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n          exec(code_obj, self.user_global_ns, self.user_ns)\n        File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/381265868.py\", line 21, in <module>\n          test_layer(final)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n          return fn(*args, **kwargs)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n          outputs = call_fn(inputs, *args, **kwargs)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n          return fn(*args, **kwargs)\n        File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\", line 152, in call\n          return self._broken_call(tensor, depth, *args)\n        File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\", line 50, in _broken_call\n          if scratchspace.shape == () or int(scratchspace.shape[0]) == max(-self._d_depth, 0):\n        File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\", line 62, in _broken_call\n          for index in tf.range(int(tensor.shape[0])):\n        File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\", line 68, in _broken_call\n          result, scratch = self._broken_call(item, bypass)\n    \n    The tensor <tf.Tensor 'while/PartitionedCall_1:0' shape=(None,) dtype=float32> cannot be accessed from FuncGraph(name=_broken_call, id=1784773312368), because it was defined in FuncGraph(name=while_body_7479, id=1784971719632), which is out of scope.\n\n\nCall arguments received:\n  • x=['<tf.RaggedTensor [[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0],\\n  [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]],\\n [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]]>', '<tf.RaggedTensor [[0.0, 0.0]]>']\n  • args=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7984/381265868.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mtest_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mtest_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_broken_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInaccessibleTensorError\u001b[0m: Exception encountered when calling layer \"test_17\" (type Test).\n\nin user code:\n\n    File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\", line 82, in _broken_call  *\n        tensor_output = tf.ragged.stack(tensor_output, axis=0)\n\n    InaccessibleTensorError: <tf.Tensor 'while/PartitionedCall_1:0' shape=(None,) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'while/PartitionedCall_1:0' shape=(None,) dtype=float32> was defined here:\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n          return _run_code(code, main_globals, None,\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\runpy.py\", line 87, in _run_code\n          exec(code, run_globals)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n          app.launch_new_instance()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n          app.start()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 667, in start\n          self.io_loop.start()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n          self.asyncio_loop.run_forever()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n          self._run_once()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n          handle._run()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\events.py\", line 80, in _run\n          self._context.run(self._callback, *self._args)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 456, in dispatch_queue\n          await self.process_one()\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 445, in process_one\n          await dispatch(*args)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 352, in dispatch_shell\n          await result\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 647, in execute_request\n          reply_content = await reply_content\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 345, in do_execute\n          res = shell.run_cell(code, store_history=store_history, silent=silent)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n          return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2898, in run_cell\n          result = self._run_cell(\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2944, in _run_cell\n          return runner(coro)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n          coro.send(None)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3169, in run_cell_async\n          has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_ast_nodes\n          if (await self.run_code(code, result,  async_=asy)):\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n          exec(code_obj, self.user_global_ns, self.user_ns)\n        File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/381265868.py\", line 21, in <module>\n          test_layer(final)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n          return fn(*args, **kwargs)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n          outputs = call_fn(inputs, *args, **kwargs)\n        File \"c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n          return fn(*args, **kwargs)\n        File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\", line 152, in call\n          return self._broken_call(tensor, depth, *args)\n        File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\", line 50, in _broken_call\n          if scratchspace.shape == () or int(scratchspace.shape[0]) == max(-self._d_depth, 0):\n        File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\", line 62, in _broken_call\n          for index in tf.range(int(tensor.shape[0])):\n        File \"C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_7984/1382995202.py\", line 68, in _broken_call\n          result, scratch = self._broken_call(item, bypass)\n    \n    The tensor <tf.Tensor 'while/PartitionedCall_1:0' shape=(None,) dtype=float32> cannot be accessed from FuncGraph(name=_broken_call, id=1784773312368), because it was defined in FuncGraph(name=while_body_7479, id=1784971719632), which is out of scope.\n\n\nCall arguments received:\n  • x=['<tf.RaggedTensor [[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0],\\n  [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]],\\n [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]]>', '<tf.RaggedTensor [[0.0, 0.0]]>']\n  • args=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "rag = tf.ragged.stack([tf.zeros([10, 2]),tf.zeros([10,1])])\n",
    "indication = tf.ragged.stack([tf.zeros([2])])\n",
    "final = [rag, indication]\n",
    "\n",
    "print(rag)\n",
    "print(indication)\n",
    "\n",
    "test = staticmethod(tf.RaggedTensor.to_tensor)\n",
    "class t():\n",
    "    pass\n",
    "\n",
    "\n",
    "class Test(Base):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x, *args):\n",
    "        return x, None\n",
    "        \n",
    "    \n",
    "test_layer = Test()\n",
    "test_layer(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a96b8d96",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Base' has no attribute 'pack'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7984/2409721615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mragged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mragged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mragged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Base' has no attribute 'pack'"
     ]
    }
   ],
   "source": [
    "Base.pack(None, tf.ragged.stack([[3]]), tf.zeros([10, 3, 4, 5, 6]))\n",
    "Base.pack(None, tf.ragged.stack([[3]]), tf.zeros([10, 3, 4]))\n",
    "Base.pack(None, tf.ragged.stack([[3]]), tf.zeros([10, 3, 4, 5, 6]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "80d1d1f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot index into an inner ragged dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21104/2655592316.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mragged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_getitem.py\u001b[0m in \u001b[0;36m_ragged_getitem_inner_dimensions\u001b[1;34m(rt_input, key_list)\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[1;31m# into a ragged inner dimension is problematic.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mrt_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform_row_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index into an inner ragged dimension.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m   \u001b[1;31m# Indexing a single column in a uniform inner dimension: check that the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot index into an inner ragged dimension."
     ]
    }
   ],
   "source": [
    "\n",
    "test = tf.ragged.stack([[4, 6], [4, 5]])\n",
    "test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "2e9a001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "def swapdims(tensor, dim1, dim2):\n",
    "    \"\"\"\n",
    "    A simple function capable of swapping two dimensions.\n",
    "    \n",
    "    \"\"\"\n",
    "    #Normalize the inputs, such that we are not setting negatives\n",
    "    length = len(tensor.shape)\n",
    "    dim1 = dim1 % length\n",
    "    dim2 = dim2 % length\n",
    "    \n",
    "    #Get the shape. Replace dim1 with dim2, dim2 with dim 1. This makes a permutation\n",
    "    permuter = list(range(length))\n",
    "    permuter[dim1] = dim2\n",
    "    permuter[dim2] = dim1\n",
    "    \n",
    "    #Permute and return\n",
    "    \n",
    "    return tf.transpose(tensor, permuter)\n",
    "\n",
    "#Test\n",
    "test = tf.zeros([10,3, 4])\n",
    "print(swapdims(test, -1, -2).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4ba4c116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 15), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Reshape(Base):\n",
    "    \"\"\"\n",
    "    The Reshape Class. The view layer contains code to extend\n",
    "    the standard tensor reshape mode. It is capable of reshaping the\n",
    "    last dimension of a tensors shape from one shape to another without\n",
    "    changing any prior dimensions, so long as the number of tensor entries\n",
    "    match.\n",
    "\n",
    "    The reshape Class has two modes. In functional mode, one may provide\n",
    "    a tensor, input_shape, and output_shape to the method .functional. This\n",
    "    will then execute the reshape logic. Alternatively, one can instance\n",
    "    the class with input_shape and output_shape, in which case it will behave\n",
    "    as a reshaping layer and consistently apply the same reshape.\n",
    "\n",
    "    For example, if one wanted to reshape tensor t=(4, 3, 4,2), to become (4,3,8),\n",
    "    one could either initialize a layer with Reshape((4,2), 8), then call it,\n",
    "    or apply Reshape.functional(t, (4,2), 8)\n",
    "    \"\"\"\n",
    "\n",
    "    ## Define the helper functions which functionally perform the view\n",
    "    @classmethod\n",
    "    def functional(cls, tensor, input_shape, output_shape):\n",
    "        \"\"\"\n",
    "        A function to change a tensor with last dimensions\n",
    "        input shape to last dimensions output shape.\n",
    "\n",
    "        For instance, given (4, 3, 4,2), reshape (4,2) to (8)\n",
    "        resulting in (4, 3, 8). Expects tensor to be tensorflow vectors.\n",
    "\n",
    "\n",
    "\n",
    "        :param tensor: The tensor to reshape\n",
    "        :param input_shape: The shape of the input dimensions which we wish to handle. Can be an integer, or a list\n",
    "          of integers.\n",
    "        :param output_shape: The shape of the output dimensions we wish to end up in\n",
    "        :return: The reshaped tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        #Handle raw integer inputs\n",
    "        if not isinstance(input_shape, Iterable):\n",
    "            input_shape = [input_shape]\n",
    "        if not isinstance(output_shape, Iterable):\n",
    "            output_shape = [output_shape]\n",
    "\n",
    "        # Perform reshape. Apply FAFP error handling.\n",
    "        try:\n",
    "            slice_length = len(input_shape)  # Find out how many dimensions will be dynamic\n",
    "            static_shape, replacement_shape = tensor.shape[:-(slice_length)], tensor.shape[-(\n",
    "                slice_length):]  # Slice apart the input shape.\n",
    "            reshape = (*static_shape, *output_shape)  # replace the output shape\n",
    "            return tf.reshape(tensor, reshape)  # reshape and return\n",
    "        except Exception as err:\n",
    "\n",
    "            # Perform input verification, and figure out where the failure happened\n",
    "            msg = \"While attempting to reshape, I found an error: %s. \\n\" % err\n",
    "            msg = msg + \"I was attempting to reshape\" + str(input_shape) +  \" to \" + str(output_shape) + \"\\n\"\n",
    "            msg = msg + \"Holmes adds that: 'Dear Watson, %s'\" % cls._autopsy(tensor, input_shape, output_shape)\n",
    "            raise Exception(msg)\n",
    "\n",
    "    @classmethod\n",
    "    def _autopsy(cls, tensor, input_shape, output_shape):\n",
    "        \"\"\"\n",
    "        A helper function to analyze what went wrong after an exception is raised\n",
    "\n",
    "        :param tensor: The input tensor\n",
    "        :param input_shape: The input shape\n",
    "        :param output_shape: The output shape\n",
    "        :return: A string representing anything I found that is wrong.\n",
    "        \"\"\"\n",
    "\n",
    "        if not tf.is_tensor(tensor):\n",
    "            return \"tensor input was not a torch tensor\"\n",
    "\n",
    "        try:\n",
    "            input_shape = np.array(input_shape, dtype=np.int32)\n",
    "            output_shape = np.array(output_shape, dtype=np.int32)\n",
    "        except:\n",
    "            return \"Either input or output shape were unclean. They did not cast to int array\"\n",
    "\n",
    "        if len(input_shape.shape) > 1:\n",
    "            return \"Input shape had more dimensions than one. Should be an int, or list of ints\"\n",
    "        if len(output_shape.shape) > 1:\n",
    "            return \"Output shape had more dimensions than one. Should be an int, or list of ints\"\n",
    "\n",
    "        if np.prod(input_shape) != np.prod(output_shape):\n",
    "            return \"The number of input tensor units was %s, and out tensor units was %s. These are not compatible\" \\\n",
    "                   % (np.prod(input_shape), np.prod(output_shape))\n",
    "\n",
    "        try:\n",
    "            tensor + torch.zeros([*input_shape])\n",
    "        except:\n",
    "            \"Input shape and tensor shape are different\"\n",
    "\n",
    "        return \"Could not find any additional information\"\n",
    "\n",
    "    ### Develop the stateful logic for that version of access\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._input_shape = input_shape\n",
    "        self._output_shape = output_shape\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        output =  self.functional(tensor, self._input_shape, self._output_shape)\n",
    "        return output \n",
    "        \n",
    "\n",
    "    \n",
    "#test\n",
    "\n",
    "test_vec = tf.zeros([10,3,5])\n",
    "Reshape.functional(test_vec, (3, 5), 15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Base):\n",
    "    \"\"\"\n",
    "    \n",
    "    The Linear class is designed to perform Dense mappings between entries. \n",
    "    \n",
    "    It has several additional extensions to the standard dense case. \n",
    "    \n",
    "    First, it is the case that input tensor size is mutable. Within the specified limits, it\n",
    "    is possible to provide a tensor that is smaller than specified, in which case only the relevant dimensions\n",
    "    are engaged.  This works, so long as it is the case that the tensor dimensions does not exceed the \n",
    "    capacity of the parameter backend. The output, however, will always be the specified width\n",
    "    \n",
    "    Second, the layer is capable of performing complex-shape to complex shape mappings. For instance, one may\n",
    "    specify\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d8720b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Local(Base):\n",
    "    \"\"\"\n",
    "    \n",
    "    Create a locally focused view of a tensor, using the given dimension. The local view will end up on the end\n",
    "    of the tensor. The dimension will almost certaintly also be resized by this process.\n",
    "    \n",
    "    Requires an input in (..., item, channel) format to function correctly. \n",
    "    Returns an output in (..., item_different, local, channel) format.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_width, striding, dilation, target_dim, new_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self._kernel_length = kernel_width*dilation \n",
    "        self._kernel_width = kernel_width \n",
    "        self._striding = striding\n",
    "        self._dilation = dilation\n",
    "        \n",
    "        #Names\n",
    "        \n",
    "        self._target_dim = target_dim\n",
    "        self._new_dim = new_dim\n",
    "        \n",
    "  \n",
    "        \n",
    "    def forward(self, stream, word_spec, *args):\n",
    "        \"\"\"\n",
    "        The logic behind the local tensor. Creates, then performs, a vectorized glimpse, adjusts\n",
    "        the mean lengths, and returns the localized result.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        channel_dim, _ = word_spec[\"channel\"]\n",
    "        item_length = stream.shape[-2]\n",
    "        \n",
    "        \n",
    "        #This is a little bit complex, so deserves some explanation. The extract_glimpse function requires \n",
    "        #an input in (batch, row, column, channel) format. It will then extract, based on the input \"glimpse offset\" parameter\n",
    "        #views of the input, with one view for each batch. \n",
    "        \n",
    "        #The input is expected to be in a (...batch_stuff, item, channel) format. The plan is to reformat\n",
    "        #this into a (1, batch_dim_collapsed, item, channel) tensor, then repeat the new batch dim, the one, \n",
    "        #the number of times required to get a complete view.\n",
    "        \n",
    "        #the stream batches is collapsed into the row column, and this tensor is then repeated for the appropriate number of\n",
    "        #units on the batch column to provide a unique tensor for each view. After this, it becomes possible to do a vectorized\n",
    "        #select, then transpose the batch and local dimension, and restore the original shape.\n",
    "        \n",
    "        \n",
    "        #Caclulate the glimpse size and glimpse offset. This is done, taking into account the needs for striding.\n",
    "        \n",
    "        unstrided_length = stream.shape[-2] - self._kernel_length\n",
    "        glimpse_offsets = tf.range(0, unstrided_length, self._striding)\n",
    "        glimpse_offsets = tf.stack([tf.zeros_like(glimpse_offsets), glimpse_offsets], axis=-1) #Final offsets\n",
    "        \n",
    "        final_item_length = glimpse_offsets.shape[0] #Length of final view.\n",
    "        item_difference = stream.shape[-2] - final_item_length #Difference between initial and final state. Used to update word length\n",
    "        glimpse_size = tf.constant([final_item_length, self._kernel_length]) #Undilated kernel width. Also, contains logic to perform complete slice.\n",
    "        \n",
    "        #Create the needed tensor\n",
    "        \n",
    "        shape = [*stream.shape[-2:]] #Get a copy of the streams last two dimensions.\n",
    "        shape.insert(0, -1) #Insert a collapse direction on the column channel\n",
    "        shape.insert(0, 1) #Insert a unmodify direction on the batch channel\n",
    "        tensor = tf.reshape(stream, shape) #Reformat tensor. (1, batch, item, channel).\n",
    "        tensor = tf.repeat(tensor, final_item_length, axis=0) #Repeat. (final_item, batch, item, channel)\n",
    "        \n",
    "        #Create the local glimpses. Perform any dilations\n",
    "\n",
    "        local_tensor = tf.image.extract_glimpse(tensor, glimpse_size, glimpse_offsets, centered=False, normalized=False) # (final_item, batch, raw_local, channel)\n",
    "        local_tensor = local_tensor[..., ::self._dilation, :] # Apply dilation. (final_item, batch, local, channel)\n",
    "\n",
    "        \n",
    "        #Restore to standard format. \n",
    "        local_tensor = swapdims(local_tensor, 0, 1) #swap to (batch, final_item, local, channel) format\n",
    "        local_tensor = tf.identity(local_tensor) #Ensure the data buffer is reset, with batch on the outside.\n",
    "        \n",
    "        shape = [*stream.shape[:-2], *local_tensor.shape[-3:]] #Stitch together original format\n",
    "        local_tensor = tf.reshape(local_tensor, shape) #expand back to original format.\n",
    "        \n",
    "        #Tensor is now in (..., final_item, local, channel) format. \n",
    "        \n",
    "        #Finally, adjust the mean length and return\n",
    "        \n",
    "        mean_length = mean_length-self._kernel_length\n",
    "        return local_tensor, mean_length\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3ddb17cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile\n",
      "Tensor(\"inputs:0\", shape=(None, None, None), dtype=float32)\n",
      "None\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 1.03 ms\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d47c16b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 0.  1.  2.  3.]\n",
      "   [ 4.  5.  6.  7.]\n",
      "   [ 8.  9. 10. 11.]\n",
      "   [12. 13. 14. 15.]\n",
      "   [16. 17. 18. 19.]]\n",
      "\n",
      "  [[20. 21. 22. 23.]\n",
      "   [24. 25. 26. 27.]\n",
      "   [28. 29. 30. 31.]\n",
      "   [32. 33. 34. 35.]\n",
      "   [36. 37. 38. 39.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  1.  2.  3.]\n",
      "   [ 4.  5.  6.  7.]\n",
      "   [ 8.  9. 10. 11.]\n",
      "   [12. 13. 14. 15.]\n",
      "   [16. 17. 18. 19.]]\n",
      "\n",
      "  [[20. 21. 22. 23.]\n",
      "   [24. 25. 26. 27.]\n",
      "   [28. 29. 30. 31.]\n",
      "   [32. 33. 34. 35.]\n",
      "   [36. 37. 38. 39.]]]], shape=(2, 2, 5, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[ 0.  1.  2.  3.]\n",
      "   [ 4.  5.  6.  7.]]\n",
      "\n",
      "  [[20. 21. 22. 23.]\n",
      "   [24. 25. 26. 27.]]]\n",
      "\n",
      "\n",
      " [[[ 4.  5.  6.  7.]\n",
      "   [ 8.  9. 10. 11.]]\n",
      "\n",
      "  [[24. 25. 26. 27.]\n",
      "   [28. 29. 30. 31.]]]], shape=(2, 2, 2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test = tf.range(40)\n",
    "test = tf.reshape(test, [2, 5, 4])\n",
    "test = test[tf.newaxis, :, :]\n",
    "test = tf.cast(test, tf.float32)\n",
    "test = tf.repeat(test, 2, axis=0)\n",
    "\n",
    "print(test)\n",
    "sizes = [2, 2]\n",
    "offsets = tf.constant([[0, 0], [0, 1]], dtype=tf.float32)\n",
    "\n",
    "print(tf.image.extract_glimpse(test, sizes, offsets, centered=False, normalized=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c5720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37e8b495",
   "metadata": {},
   "source": [
    "# Dynamic\n",
    "\n",
    "Some of the methods of dynamic\n",
    "\n",
    "Ideas:\n",
    "\n",
    "* Traveling kernel process\n",
    "* Frozen pivot process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d12fc",
   "metadata": {},
   "source": [
    "# Dynamic Item Length\n",
    "\n",
    "One of the major issues in traditional NLP is the need to statically define the shape of an output. This is an attempt to allow the machine to handle this.\n",
    "\n",
    "\n",
    "## Process:\n",
    "\n",
    "* Total vector length does, in fact, remain constant for calculation purposes. We do not need to keep recalculating the \n",
    "* graph.\n",
    "* Items are formatted in [0, item, 0, item, ...] format after embedding process is complete.\n",
    "* Further layers may inject additional information into internal channels\n",
    "* Subsequent \"resize\" layer will take the product of \"decide\" with process vector:\n",
    "    [item, item2, item3, item4]*[decide]. This reduces it down to a scalar. \n",
    "* The scalar is used to make a decision. Positive scalars are deemed words. Negative ones, ignored.\n",
    "* The positive cases are extracted, and reembedded as [0, item, 0, item, ...]. \n",
    "* This proceeds until end, where loss is calculated. \n",
    "\n",
    "## Positive\n",
    "\n",
    "## Loss\n",
    "\n",
    "* Loss is calculated as hinge loss. \n",
    "* If too many words are active, all active words are met with a rejection penalty. This penalty is equivalent to the inverse of decision* inverse, meaning word which are almost off will shut off faster than words for which we are more certain.\n",
    "* If too few words are active, the same happens in reverse, with emphasis on words that are off. \n",
    "* Backprop or manual gradient adjustment may be necissary.\n",
    "\n",
    "* Items are always embedded as [0, item, 0, item, 0, item,...] tensors after passing through a special layer.\n",
    "* This layer in turn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e81f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicBase():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70088f31",
   "metadata": {},
   "source": [
    "\n",
    "#  Trainable Dynamic Item Resize by the Technique of Virtual Policy Gradients.\n",
    "\n",
    "## Dynamic Resize\n",
    "\n",
    "Dynamic resize consists of allowing the machine learning algorithm to smoothly add or remove \"words\" or embedding items into it's stream. It is also the case that this should operate according to primary gradient descent to be at all practical. Many, Many challenges are faced here. \n",
    "\n",
    "* Smoothness. It should be the case that adding and removing words is numerically \"smooth.\" This means changing the word configuration should NOT result in a large jump \n",
    "* Gradients. Gradients should both exist, and be useful at changing the number of words.\n",
    "* Loss. Loss should be reasonably easy to impliment given the objectives\n",
    "* Computation. Computation should not be significantly greater than standard methods. Additionally, it should preferably be smaller. \n",
    "* Per layer size. It should be the case that the model can decide, per layer, how wide it wants its embeddings to run.\n",
    "\n",
    "Accomplishing this is by no means an easy feat. Nonetheless, to my suprise, it is possible. The technique to accomplish this is virtual gradients.\n",
    "\n",
    "\n",
    "Assumption is: Embeddings which are near each other are relavent to each other.\n",
    "\n",
    "\n",
    "\n",
    "## Frozen Pivot\n",
    "\n",
    "Frozen pivot gradients is a technique to allow a calculated selector of some sort, in this case the WordLength, to impliment parallel possibilities even in difficult circumstances such as vectors whose sizes are changing. It consists of\n",
    "a fairly simple update function, the \"frozen pivot\", which controls how much to prioritize each possibilties. It will only work with ordered possibilities. \n",
    "\n",
    "**Forward pass**\n",
    "\n",
    "On each layer per evaluation, we calculate a WordLength. This WordLength is calculated by running the incoming tensors through several layers, and represents the mean word lenght we expect to output. After this, we use it to calculate new numbers +-epsilon, etc. This is then known as the compression tensor, and represents how many words the next layer should have. \n",
    "\n",
    "After this we turn the Representation tensor into the Proxy tensor. First, we develop a linear equation centered at the mean and governed by y =(m_d-m) * x + m_d, where m is the graph mean and m_d is the mean, but detatched. This is then first fed through a softmax function, creating the Proxy tensor. Also, do note it is absolutely essencial that the gradient be halted on m_d. Finally, we use the compression tensor to reshape the input, then run them through layers, and finally combine them with probabilities given by the Proxy tensor. The output, and the WordLength are then returned. \n",
    "\n",
    "Note that at this point, we are actually done. Any attempt by the backprop algorithm to modify Proxy will inevitably ONLY have the ability to modify the mean. So long as a longer, or shorter, tensor allows better results the model should see it and expand or contract as needed. Additionally, the mean itself will also be pushed to be smaller, or larger, by the regularization parameters.\n",
    "\n",
    "**Loss And Regularization**\n",
    "\n",
    "Primary loss consists of the normal calculation process.\n",
    "\n",
    "Meanwhile, several additional regularization and loss terms are required to make the algorithm work.\n",
    "\n",
    "* A endpoint loss for yelling at the model for having too few or too many words. \n",
    "* A regularization loss, L2?, on the mean for each layer. Encourages compression.\n",
    "* A strong penalty for any mean whose words are dropping below zero.\n",
    "* Dropout on the mean construction layer. Introduces jitter which ensures the model width does not lock up.\n",
    "\n",
    "\n",
    "The regularization loss is a simple matter. Gather the means, and scale. The endpoint loss will also, it turns out, be fairly easy. Simply go ahead ahead and form a loss with respect to the WordLength, which is returned for that exact purpose, and should dominate. All other behavior can be left to the model, which should quickly rescale it's output to the appropriate size, then slowly fill in model sizes as the process proceeds.\n",
    "\n",
    "\n",
    "\n",
    "**Expand-Mean Resize**\n",
    "\n",
    "The LCM-Mean Resize operates as follows. \n",
    "\n",
    "t\n",
    "\n",
    "\n",
    "\n",
    "The LCM-Mean Resize is an algorithm which attempts to preserve local context between embedding entries. It operates as follows. Let there be a length L that the embeddings currently exist as, and a length K which is the target size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a574c",
   "metadata": {},
   "source": [
    "## Resize Transfomer\n",
    "\n",
    "The action of resize needs equal consideration to the action of training the resize. While Frozen Pivot may make training a rescale possible, consideration still needs to be given to how to make it actually happen. This is where tranformers and the resize logic comes into play.\n",
    "\n",
    "Transformers will be utilized to resize the layers, but that still leaves a problem: Generating the queue. While it will be the case that the Key and Value can be generated from the incoming stream directly, the Queue needs to be the length of the \n",
    "output to work correctly. \n",
    "\n",
    "\n",
    "\n",
    "**Queue**\n",
    "\n",
    "The queue is generated by performing an interleave expansion, a view, and then either a max or a mean. \n",
    "\n",
    "Let K be the initial embeddings size, and L be the target embeddings size. Perform a repeat interleave along embeddings of length L. The embeddings now cleanly divide into L. Go and perform a view on the embeddings, looking at (L, K). It is now the case that one dimension is the right shape. \n",
    "\n",
    "At this point, any summarizing statistic can be performed on the last embedding dimension to get something of length L. Mean is predicted to be particularly helpful when operating locally, but max might be useful as well.\n",
    "\n",
    "**Transformer**\n",
    "\n",
    "With a Queue tensor made, Transfomer can be executed. Take the Queue, take the Content, expand heads and transform. Resize is then complete. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d395ccd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 253)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m253\u001b[0m\n\u001b[1;33m    def __init__(self, internal, channel_width, channel_dim = -1, item_dim=-2, superior=None):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Penalty():\n",
    "    \"\"\"\n",
    "    \n",
    "    The penalty class is an extremely important portion of the\n",
    "    dynamic layer. It is responsible for collecting a penalty \n",
    "    forcing a corrolation between selectors. Several types of penalty exist. \n",
    "    Currently, these are Corrolation and Jacobian.\n",
    "\n",
    "    Corrolation computes the logits version of a corrolation, then uses hinge\n",
    "    loss to compute a penalty. It is required for the corrolation to be above \n",
    "    a certain amount for no penalty to exist\n",
    "    \n",
    "    Jacobian penalty, meanwhile, computes the Jacobian between \n",
    "    the selections. The Jacobian entries then become an indication of corrolation.\n",
    "    Any entry which is below a certain threshold is then penalized\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def covarience(a, b):\n",
    "        \n",
    "        #flatten most of the dimensions into a single batch. Calculate N\n",
    "        a = a.view([-1, a.shape[-1]]) \n",
    "        b = b.view([-1, b.shape[-1]])\n",
    "        n = a.shape[-2]\n",
    "        \n",
    "        #Calculate the covarience between the channel items.\n",
    "        \n",
    "        cov = (a-a.mean(-1).unsqueeze(-1))(b - b.mean(-1).unsqueeze(-1))/n\n",
    "        \n",
    "        #Return the covarience.\n",
    "        return cov \n",
    "\n",
    "    def __init__(self, variety=\"Covarience\", threshold =0.2):\n",
    "        \"\"\"\n",
    "        The init function. \n",
    "        \n",
    "        Variety may be \"Covarience\" or \"Jacobian.\" Threshold controls\n",
    "        how strong the hinge loss is.\n",
    "        \n",
    "        \"\"\"\n",
    "    def __call__(self, select_old, select_new, penalty)\n",
    "\n",
    "class Dynamic()\n",
    "    \"\"\"\n",
    "    \n",
    "    Dynamic is a metalayer containing the layers and functions\n",
    "    needed to do Dynamic Resize in functional, procedural, and \n",
    "    object oriented manners. \n",
    "    \n",
    "    The three primary classes in dynamic are Select, \n",
    "    Encode, and Scatter. These, respectively, are in charge of selecting words to move forward with,\n",
    "    encoding the unselected residuals back into the selection tensor, and then scattering the output into a \n",
    "    InterspacedWord. The classes are part of the metaclass, and may be initialized individually - Additionally,\n",
    "    it is the case that the subclasses have a functional method, which can be used to execute the logic directly.\n",
    "    \n",
    "    The driving observation which makes this layer possible is that it is possible to built a follow up operator such \n",
    "    that so long as the transition between having x and x+1 tensors occurs when the magnitude of the tensor is zero, there\n",
    "    is little to no influence on the output, driving it's relation into a linear zone. To do this, we smooth the \n",
    "    output with a no-bias local transformer, though both biased and unbiased are included for the sake of experimentation.    \n",
    "    \"\"\"\n",
    "    ## Classes. To reduce clutter to some degree, I store a number of classes inside the larger class.\n",
    "    \n",
    "\n",
    "    ### Helper: These are called by primary logic functions. ###\n",
    "    \n",
    "    @staticmethod\n",
    "    def lcm_mean_reshape(tensor, dim, length):\n",
    "        \"\"\"\n",
    "\n",
    "        Perform a weightless resize, attempting to preserve some degree of\n",
    "        local information\n",
    "\n",
    "        \"\"\"\n",
    "        #place the dim in the correct location\n",
    "\n",
    "        tensor = tensor.swapdims(-1, dim)\n",
    "\n",
    "        #Calculate the lcm of the length and the dimension length. Use this to calculate kernel size. \n",
    "\n",
    "        dim_length = tensor.shape[-1]\n",
    "        lcm = math.lcm(dim_length, length)\n",
    "        expansion_multiplier = lcm//dim_length\n",
    "        kernel_size = lcm//length\n",
    "\n",
    "\n",
    "        #Expand kernel to size. Perform local view of appropriate dimensions. Average views. return.\n",
    "        kernel = tensor.repeat_interleave(expansion_multiplier, dim=-1)\n",
    "        width = kernel_size - 1\n",
    "        stride = width\n",
    "\n",
    "        kernel = LocalView.functional(kernel, 0, width, kernel_size, 1, dim=-1, pad=True)\n",
    "\n",
    "        print(kernel.shape)\n",
    "        kernel = kernel.mean(dim=-1).squeeze()\n",
    "\n",
    "        output = kernel.swapdims(-1, dim)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    ### Primary Logic: These are called in sequence from forward ###\n",
    "    \n",
    "    def select(cls, tensor):\n",
    "        \"\"\"\n",
    "        The select metod is a functional method designed to produce a selector given a cutoff\n",
    "        function. \n",
    "        \n",
    "        A selector is a tensor with a single scalar value for each entry in channel dimension. This value\n",
    "        should be positive, if the word will be retained, or negative, if not.\n",
    "        \"\"\"\n",
    "        return self._cutoff(selector)\n",
    "    \n",
    "    \n",
    "  \n",
    "    @classmethod\n",
    "    def segment(cls, tensor, selector):\n",
    "        \"\"\"\n",
    "        The segment method is responsible for taking in a tensor and a selector, then utilizing it \n",
    "        to create a set of rejected and accepted tensors. \n",
    "        \n",
    "        It does this with masks and sort. Tensors have a decision made about them, then masked into two groups and\n",
    "        are sorted by which decisive catagory they fell into. After this, they are sliced to the longest tensor,\n",
    "        and the two groups are returned. \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Develop the sort tensor, which represents retained values with a one. \n",
    "        #Also, multiply the tensor by the selector, ensuring a smooth transition region.\n",
    "        \n",
    "        sort_in = torch.where(selector > 0, 1, 0)\n",
    "        tensor = selector*tensor\n",
    "\n",
    "\n",
    "\n",
    "        #Find the number of active, off words. Find their indices using sort. Figure out the length of the output\n",
    "        #vector which will be needed.\n",
    "\n",
    "        active = (selector > 0).sum().type(torch.int32)\n",
    "        off = (selector < 0).sum().type(torch.int32)\n",
    "        _, primary_indices = torch.sort(sort_in, dim=-2, descending=True, stable=True) #Should not change order. Active first.\n",
    "        _, rejection_indices = torch.sort(sort_in, dim=-2, descending=False, stable=True)\n",
    "\n",
    "\n",
    "        #Create the reformatted generation tensor. Give the program the ability to influence how much a word is on. Do this\n",
    "        #by multiplying the tensor by the selector, and hence encouraging vectors to be zero when near off. Then split into\n",
    "        #two tensors; the primary and residual tensor.\n",
    "\n",
    "        primary = torch.where(selector > 0, tensor, 0)\n",
    "        rejections = torch.where(selector < 0, tensor, 0)\n",
    "\n",
    "        #Sort the primary tensor. Cut it out. Sort the residual tensor. Cut them out. Then use a lcm-mean to embed\n",
    "        #the residual information back into the primary tensor, allowing gradient descent to continue acting upon it.\n",
    "\n",
    "        #It should be noted that this particular trick is VERY important. Since residuals are \"relatively local\" and since\n",
    "        #the relu trick means that getting closer to the decision threshold shuts off each tensor, an output item can turn \"off\"\n",
    "        #more either by reducing it's actual input, or increasing how strongly off it's residual sources are. Likewise, it may turn \n",
    "        #more strongly on both by increasing it's magnitude, AND by bringing a residual instance more online. \n",
    "\n",
    "        #Furthermore, since at weak scales it is the case that under near-zero conditions linear effects dominate,\n",
    "        #per the taylor expansion, the transition will be quite smooth, sans any trouble due to positional embeddings.\n",
    "\n",
    "        primary = primary[primary_indices]\n",
    "        primary = primary[..., :active, :]\n",
    "\n",
    "        rejections = rejections[residual_indices]\n",
    "        rejections = rejections[..., :off, :]\n",
    "        \n",
    "        return primary, rejections\n",
    "        \n",
    "\n",
    "    def scatter(self, scatter_tensor):\n",
    "        \"\"\"\n",
    "        This is the  scatter method.\n",
    "        \n",
    "        The scatter method takes in a tensor to scatter, and the amount of interspace, \n",
    "        then proceeds to make a wordchange tensor with the scatters throughout it. It does\n",
    "        not, however, retain the residual information.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        interspace = self._interspace\n",
    "        #Put tensor in item, channel dimension format\n",
    "\n",
    "        scatter_tensor = scatter_tensor.swapdims(-1, channel_dim)\n",
    "        scatter_tensor = scatter_tensor.swapdims(-2, item_dim)\n",
    "\n",
    "        #Calculate, then create, a tensor big enough to be returned. The tensor should have the \n",
    "        #properties that it has a distance of interspace before the first word, then alternate words\n",
    "        #and interspace distance\n",
    "\n",
    "        scatter_length = scatter_tensor.shape[-2]\n",
    "        output_length = interspace + scatter_length + scatter_length*interspace\n",
    "        build_shape = scatter_length.shape[:]\n",
    "        build_shape[-2] = output_length()\n",
    "\n",
    "        output = torch.zeros(built_shape)\n",
    "\n",
    "        #Perform a strided scatter into output. Start after the first interspace, and then place the entire scatter\n",
    "        #tensor into the output, striding by the interspace plus one distance\n",
    "\n",
    "        output[(interspace+1)::(interspace+1)] = scatter_tensor\n",
    "\n",
    "        #Rearrange output, then return\n",
    "\n",
    "        output = output.swapdims(-1, channel_dim)\n",
    "        output = output.swapdims(-2, item_dim)\n",
    "        return output\n",
    "    @classmethod\n",
    "    def _residual_encode(cls, tensor, residual, alpha):\n",
    "        \"\"\"\n",
    "        \n",
    "        The residual encode method is responsible for giving gradient descent something to push against when \n",
    "        encoding.\n",
    "        \n",
    "        It takes in the output tensor, and the rejections, then combines them such that \n",
    "        a weak output occurs even on words deemed to be \"off,\" in the manner of a leaky relu\n",
    "        \n",
    "        \"\"\"\n",
    "        required_length = tensor.shape[-2]\n",
    "        reformatted_residuals = self.lcm_mean_reshape(residual, dim=-2, required_length)    \n",
    "        return tensor+alpha*reformatted_residuals\n",
    "\n",
    "    def __init__(self, internal, channel_width, cutoff, oenalty, smooth,\n",
    "                 interspace=1, channel_dim=-1, item_dim = -2, alpha=0.01, beta=0.01):\n",
    "        \n",
    "        #Start up torch\n",
    "        super().__init__()\n",
    "        \n",
    "        #\n",
    "        self._channel_dim = channel_dim\n",
    "        self._item_dim = item_dim\n",
    "        \n",
    "        #Store layers.\n",
    "\n",
    "        self.internal = internal\n",
    "        self.smooth = smooth\n",
    "        self._cutoff = cutoff\n",
    "        self.penalty = penalty\n",
    "        \n",
    "        #Store constants\n",
    "        self._interspace = interspace\n",
    "        self._alpha = alpha\n",
    "        self._beta = beta\n",
    "        \n",
    "        #Create layernorm\n",
    "        \n",
    "        self._LayerNorm = nn.LayerNorm((channel_width))\n",
    "        \n",
    "    \n",
    "    def process(self, stream, selection):\n",
    "        \"\"\"\n",
    "        process takes in a tensor, and a selection. It then returns a tensor and a selection.\n",
    "        \"\"\"\n",
    "        \n",
    "        ## Perform primary processing. \n",
    "        \n",
    "        tensor, residual = self.segment(stream, selection) #Segment\n",
    "        tensor = self.scatter(tensor) #Scatter\n",
    "        tensor = self.smooth(tensor) #Smooth.\n",
    "        tensor = self.internal(tensor) #Sublayer logic\n",
    "        tensor = self.residual_encode(tensor, residual, -self._alpha) #Off residual passthrough\n",
    "        tensor = self.residual_encode(tensor, stream, self._beta) #On residual passthrough\n",
    "        tensor = self.LayerNorm(tensor)\n",
    "        \n",
    "        #Create selection\n",
    "        select = self.select(tensor)\n",
    "        \n",
    "        #Return outputs \n",
    "        return tensor, select\n",
    "        \n",
    "    def forward(self, stream, select, penalty=None):\n",
    "        \n",
    "        tensor, selector = self.process(stream, select)\n",
    "        penalty += self.penalty(stream, selector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d23fd9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.zeros((10, 4, 3))\n",
    "tensor.masked_select(torch.tensor([True, False, True])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea071d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 10])\n",
      "torch.Size([23, 23])\n",
      "torch.Size([3, 20])\n",
      "tensor([[-0.3247, -0.2765,  0.1318, -0.3361,  0.3723,  0.1311, -0.1954,  0.5786,\n",
      "          0.4519, -0.2662, -0.0466,  0.5212, -0.1485, -0.3027,  0.6917, -0.0187,\n",
      "         -0.5208,  0.1179,  0.0363, -0.0585],\n",
      "        [-0.0030, -0.0390,  0.2266,  0.0653,  0.1567, -0.1725, -0.2575,  0.0119,\n",
      "          0.4352, -0.0791, -0.0950, -0.1559,  0.2093, -0.7495,  0.1910,  0.6346,\n",
      "         -0.2099,  0.0363,  0.0903, -0.0086],\n",
      "        [ 0.0301,  0.1271, -0.3464,  0.3308, -0.2540,  0.0055, -0.2179, -0.3874,\n",
      "         -0.2600, -0.0021,  0.0254, -0.3917, -0.1571, -0.1900, -0.3285, -0.0197,\n",
      "          0.3219, -0.0585, -0.0086,  0.0824]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "noise_a = 10*torch.rand([20, 10])\n",
    "noise_b = torch.rand([3, 10])\n",
    "\n",
    "def corrolating(a, b):\n",
    "    \n",
    "\n",
    "print(corrolating(noise_a, noise_b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf116e9",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727db5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed362fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28518978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9666463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552f6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1deb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0b14d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
