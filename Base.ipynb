{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Base\n",
    "\n",
    "This contains basic layers used for various functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.10.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\chris\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade torch\n",
    "\n",
    "import math\n",
    "import unittest\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "Core is used everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### View\n",
    "\n",
    "A better form of torch's view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DimensionalTransform\n",
    "\n",
    "Many, many functions of pytorch expect the dimension of interest to be on the last dimension, or at least located relative to it. I do not expect that. To accomodate nonstandard data loadouts, this class contains some functions to transform a tensor to have a particular dimension last, let you transparently do things to it, including increasing the number of dimensions, and transform it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def craft_transforms(self, dim):\n",
    "    \"\"\"\n",
    "    A method to calculate, then return, fuss free transforms and inverse transforms\n",
    "    to allow editing of a particular dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def roll_dims(tensor, direction):\n",
    "\n",
    "      total_dims = len(tensor.shape)\n",
    "\n",
    "      roll = dim %(- total_dims) #distance left to reach point from the end. Indexing starts at -1. Goes down\n",
    "      roll = - roll #Indexing now starts at 1, goes up\n",
    "      roll = (roll - 1) % total_dims #indexing now starts at 0. Goes up by roll needed.\n",
    "      roll = roll*direction\n",
    "\n",
    "      permute = torch.arange(len(tensor.shape))\n",
    "      permute = permute.roll(roll).numpy()\n",
    "      permute = [*permute]\n",
    "      return tensor.permute(permute)\n",
    "\n",
    "    #Create transform, inverse transform\n",
    "\n",
    "    transform = lambda x: roll_dims(x, 1)\n",
    "    inv_transform = lambda x: roll_dims(x, -1)\n",
    "\n",
    "    return transform, inv_transform\n",
    "\n",
    "def craft_mass_exchange(self, target_dims):\n",
    "  \"\"\"\n",
    "  A method to make transforms to swap any dimensions, in any particular order,\n",
    "  to the end of a tensor, or swap it back\n",
    "  \"\"\"\n",
    "  if isinstance(target_dims, int):\n",
    "    target_dims = [target_dims]\n",
    "\n",
    "  #Calculate permuter. Do this by identifying target ending region,\n",
    "  #target source region, then using a buffer to transfer entities\n",
    "  #between these regions.\n",
    "\n",
    "  swap_a = torch.arange(-len(target_dims), 0)\n",
    "  swap_b = target_dims\n",
    "  #Fortunately, this also builds the inverse permuter too.\n",
    "  def exchange(tensor):\n",
    "    total_dims = len(tensor.shape)\n",
    "\n",
    "    source = torch.arange(total_dims)\n",
    "    endings = [*torch.arange(-len(target_dims), 0).numpy()]\n",
    "\n",
    "    permute = source.clone()\n",
    "    permute[endings] = source[target_dims]\n",
    "    permute[target_dims] = source[endings]\n",
    "    permute = [*permute.numpy()]\n",
    "\n",
    "    #Create transforms. Return\n",
    "\n",
    "    return tensor.permute(permute)\n",
    "\n",
    "  return exchange, exchange\n",
    "\n",
    "def DimHandler(cls):\n",
    "  \"\"\"\n",
    "  A decorator to apply this logic to a particular class\n",
    "  \"\"\"\n",
    "  cls.craft_transforms = craft_transforms\n",
    "  cls.craft_mass_exchange = craft_mass_exchange\n",
    "  return cls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4, 5])\n",
      "torch.Size([1, 2, 3, 5, 4])\n",
      "torch.Size([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "test_range = torch.zeros([1, 2, 3, 4, 5])\n",
    "print(test_range.shape)\n",
    "transform, inverse = craft_mass_exchange(None, [-1, -2])\n",
    "intermediate = transform(test_range)\n",
    "print(intermediate.shape)\n",
    "final = inverse(intermediate)\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModularPadClip\n",
    "\n",
    "A layer to perform ModularPadClip. This ensures an indicated dimension, by default -1, is divisible by modulo.\n",
    "\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTmp0Iag-1dKXntJ6LUftoaagASaEAt_thOkFOVr_7XxKHOjNhAeA6bSjXcupHYsHdXYRpD8uLxREau/pub?w=960&amp;h=720\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ModularPadClip. The input parameters are:\n",
      "tensor: torch.Size([30, 67, 38, 91]), modulo: 7, variety: constant, value: 0, dim: -1\n",
      "The output shape was:\n",
      "torch.Size([35, 67, 38, 91])\n",
      "Testing ModularPadClip. The input parameters are:\n",
      "tensor: torch.Size([30, 67, 38, 91]), modulo: 7, variety: constant, value: 0, dim: -2\n",
      "The output shape was:\n",
      "torch.Size([35, 67, 38, 91])\n",
      "Testing ModularPadClip. The input parameters are:\n",
      "tensor: torch.Size([30, 67, 38, 91]), modulo: 7, variety: constant, value: 0, dim: -2\n",
      "The output shape was:\n",
      "torch.Size([35, 67, 38, 91])\n",
      "Testing ModularPadClip. The input parameters are:\n",
      "tensor: torch.Size([30, 67, 38, 91]), modulo: 5, variety: circular, value: 0, dim: -3\n",
      "The output shape was:\n",
      "torch.Size([30, 67, 38, 91])\n",
      "Testing ModularPadClip. The input parameters are:\n",
      "tensor: torch.Size([30, 67, 38, 91]), modulo: 7, variety: clip, value: 0, dim: -2\n",
      "The output shape was:\n",
      "torch.Size([28, 67, 38, 91])\n"
     ]
    }
   ],
   "source": [
    "@DimHandler\n",
    "class ModularPadClip(nn.Module):\n",
    "  \"\"\"\n",
    "  A class containing methods needed to do\n",
    "  block padclip. This enlarges, or clips, a dimension to cleanly\n",
    "  divide under modular divsion by modulo\n",
    "\n",
    "  If not instantized, the static method\n",
    "  .functional can be used to utilized the logic directly.\n",
    "\n",
    "  Else, can be instantized with values to apply that to subsequent calls.\n",
    "\n",
    "\n",
    "  Parameters are:\n",
    "\n",
    "  :param: moduol: The size of the modulo to enforce\n",
    "  :param variety: The kind of ModularPadClip to do.\n",
    "    Allowed are \"constant, reflect, replicate, circular, clip\"\n",
    "  :param value: Any auxilary values for padding purposes.\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  _pad_varieties = (\"constant\", \"circular\")\n",
    "  _clip_varieties = (\"clip\")\n",
    "\n",
    "  @classmethod\n",
    "  def functional(cls, x, modulo, variety, value, dim=-1):\n",
    "    \"\"\"\n",
    "    A function to perform block PadClip.\n",
    "\n",
    "    This will reshape the last dimension\n",
    "\n",
    "  :param x: The tensor to operate on\n",
    "  :param modulo: The size of the modulo to enforce\n",
    "  :param variety: The kind of ModularPadClip to do.\n",
    "    Allowed are \"constant, reflect, replicate, circular, clip\"\n",
    "  :param value: Any auxilary values for padding purposes.\n",
    "  :param dim: The dimension to PadClip\n",
    "    \"\"\"\n",
    "\n",
    "    #Create Transforms. Apply\n",
    "    transform, inverse = cls.craft_transforms(dim, len(x.shape))\n",
    "\n",
    "    #Begin Logic\n",
    "    x = transform(x)\n",
    "    item_length = x.shape[-1]\n",
    "\n",
    "    ##If no reshape is needed, do not perform\n",
    "    if item_length % modulo == 0:\n",
    "      output = x\n",
    "\n",
    "    ##Handle pad, clip cases\n",
    "    elif variety in cls._pad_varieties:\n",
    "\n",
    "      ###Figure out how much we need to pad the end by.\n",
    "\n",
    "      required_blocks =  (modulo+item_length)//modulo    #### Perform integer ceil\n",
    "      required_length = modulo*required_blocks #### Find required length\n",
    "      padding_length = required_length-item_length #### Find padding needs\n",
    "\n",
    "      ### Apply padding\n",
    "      if variety == \"constant\":\n",
    "        output = F.pad(x, (0, padding_length), variety, value)\n",
    "      if variety == \"circular\":\n",
    "        roll_slice = x[..., :padding_length] #Slice out first padding entries and concat to end.\n",
    "        output = torch.concat([x, roll_slice], dim=-1)\n",
    "    elif variety in cls._clip_varieties:\n",
    "      required_blocks = item_length//modulo #### integer floor\n",
    "      required_length = modulo*required_blocks #### How long the slice will be\n",
    "\n",
    "      ### Apply slice and return\n",
    "      output = x[..., :required_length]\n",
    "    else:\n",
    "      raise ValueError(\"Expected variety to be one of %s)\" % (*cls._pad_varieties, *cls._clip_varieties))\n",
    "\n",
    "    #Finish with restore\n",
    "    return inverse(output)\n",
    "  def __init__(self, modulo, variety=\"constant\", value=0, dim=-1):\n",
    "    \"\"\"\"\n",
    "\n",
    "    :param modulo: The size of the modulo to enforce\n",
    "    :param variety: The kind of ModularPadClip to do.\n",
    "      Allowed are \"constant, reflect, replicate, circular, clip\"\n",
    "    :param value: Any auxilary values for padding purposes.\n",
    "    :param dim: The dimension to PadClip\n",
    "    \"\"\"\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    #store\n",
    "\n",
    "    self._modulo = modulo\n",
    "    self._variety = variety\n",
    "    self._value = value\n",
    "    self._dim = dim\n",
    "  def forward(self, x):\n",
    "    return self.functional(x, self._modulo, self._variety, self._value, self._dim)\n",
    "\n",
    "#Unitcode\n",
    "\n",
    "##Define test function\n",
    "def TestFunc(tensor, modulo, variety, value, dim):\n",
    "  #Introduce test\n",
    "  print(\"Testing ModularPadClip. The input parameters are:\")\n",
    "  variable_message = \"tensor: %s, modulo: %s, variety: %s, value: %s, dim: %s\" % (tensor.shape, modulo, variety, value, dim)\n",
    "\n",
    "  print(variable_message)\n",
    "\n",
    "  #Run test\n",
    "  test_layer = ModularPadClip(modulo, variety, value, dim)\n",
    "  output = test_layer(tensor)\n",
    "\n",
    "  #Say results\n",
    "  print(\"The output shape was:\")\n",
    "  print(output.shape)\n",
    "\n",
    "#Perform some tests\n",
    "test_vec = torch.zeros([30, 67, 38, 91])\n",
    "TestFunc(test_vec, 7, \"constant\", 0, -1)\n",
    "TestFunc(test_vec, 7, \"constant\", 0, -2)\n",
    "TestFunc(test_vec, 7, \"constant\", 0, -2)\n",
    "TestFunc(test_vec, 5, \"circular\", 0, -3)\n",
    "TestFunc(test_vec, 7, \"clip\", 0, -2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RelPosEncoding\n",
    "\n",
    "Absolute encodings are not used here. Rather, everything is relative, and as a result a modified version of PosEncoding is needed.\n",
    "\n",
    "The standard PosEncoding using the sinosoid is utilized, with a few new twists. It is possible for the zero point, or start point, to be defined later in the sequence - allowing the clear definition of the concept of \"prior\" sections. The prior concept can be utilized to embed local block information. Or it can be made to operate in standard format instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual inspection: \n",
      "\n",
      "Performing unit test for RelPosEncoding\n",
      "Parameters are: tensorshape: torch.Size([5, 4]), init_offset: 0, eval_offset: 0, init_pos_length: 100, channel_width: 4, period: 100\n",
      "Output shape is:\n",
      "torch.Size([5, 4])\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 5.0000e-01, -2.0211e-01,  4.0451e-01,  4.9016e-01],\n",
      "        [ 1.7485e-07, -3.6972e-01,  4.7553e-01,  1.9350e-01],\n",
      "        [ 5.0000e-01,  4.7423e-01, -1.5451e-01,  4.1377e-01],\n",
      "        [ 3.4969e-07,  4.9781e-01,  2.9389e-01,  3.5685e-01]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Performing unit test for RelPosEncoding\n",
      "Parameters are: tensorshape: torch.Size([5, 4]), init_offset: 1, eval_offset: 1, init_pos_length: 100, channel_width: 4, period: 100\n",
      "Output shape is:\n",
      "torch.Size([5, 4])\n",
      "tensor([[ 5.0000e-01, -2.0211e-01,  4.0451e-01,  4.9016e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 5.0000e-01, -2.0211e-01,  4.0451e-01,  4.9016e-01],\n",
      "        [ 1.7485e-07, -3.6972e-01,  4.7553e-01,  1.9350e-01],\n",
      "        [ 5.0000e-01,  4.7423e-01, -1.5451e-01,  4.1377e-01]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Performing unit test for RelPosEncoding\n",
      "Parameters are: tensorshape: torch.Size([5, 4]), init_offset: 2, eval_offset: 1, init_pos_length: 100, channel_width: 4, period: 100\n",
      "Output shape is:\n",
      "torch.Size([5, 4])\n",
      "tensor([[ 5.0000e-01, -2.0211e-01,  4.0451e-01,  4.9016e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 5.0000e-01, -2.0211e-01,  4.0451e-01,  4.9016e-01],\n",
      "        [ 1.7485e-07, -3.6972e-01,  4.7553e-01,  1.9350e-01],\n",
      "        [ 5.0000e-01,  4.7423e-01, -1.5451e-01,  4.1377e-01]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Buffer resize: \n",
      "\n",
      "Performing unit test for RelPosEncoding\n",
      "Parameters are: tensorshape: torch.Size([10, 100, 150, 5]), init_offset: 5, eval_offset: 10, init_pos_length: 200, channel_width: 5, period: 10000\n",
      "recalculate\n",
      "Output shape is:\n",
      "torch.Size([10, 100, 150, 5])\n",
      "Performing unit test for RelPosEncoding\n",
      "Parameters are: tensorshape: torch.Size([10, 100, 150, 5]), init_offset: 5, eval_offset: 10, init_pos_length: 50, channel_width: 5, period: 10000\n",
      "recalculate\n",
      "Output shape is:\n",
      "torch.Size([10, 100, 150, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp/ipykernel_1708/3777455368.py:25: UserWarning: RelPosEncoding layer is recalculating buffer. Consider choosing better defaults if this happens often\n",
      "  warnings.warn(\"RelPosEncoding layer is recalculating buffer. Consider choosing better defaults if this happens often\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class RelPosEncoding(nn.Module):\n",
    "  \"\"\"\n",
    "  A class to perform RelPosEncoding along two dimensions.\n",
    "\n",
    "  Due to frequency being a required quantity for buffering to work,\n",
    "  this does not have a functional variety. It will, however, resize\n",
    "  its buffer if it is too small, and throw a warning when doing it.\n",
    "\n",
    "  It is, however, required that channel width stay consistent. One additional\n",
    "  complication which should also be noted is that since positional encodings\n",
    "  will be added in frequently, the combination strength is a learnable parameter.\n",
    "  \"\"\"\n",
    "\n",
    "  @property\n",
    "  def length(self):\n",
    "    return self._offset + self._standard\n",
    "  @staticmethod\n",
    "  def calculate_buffer(offset, standard_length, channel_width, period, dilation=1, suppress_errors=False):\n",
    "    \"\"\"\n",
    "    Perform a bog-standard sinosoidal posencoding calculation, with the notable\n",
    "    distinction of permitting negative item values\n",
    "\n",
    "    \"\"\"\n",
    "    if suppress_errors is False:\n",
    "      warnings.warn(\"RelPosEncoding layer is recalculating buffer. Consider choosing better defaults if this happens often\")\n",
    "\n",
    "    #Create item indices, create channel indices\n",
    "    pos_enc = torch.arange(-offset, standard_length)\n",
    "    pos_enc = pos_enc*dilation\n",
    "    channel_indices = torch.arange(channel_width)\n",
    "\n",
    "    #Unsqueeze for cross multiplication\n",
    "    pos_enc = pos_enc.unsqueeze(-1)\n",
    "    channel_indices = channel_indices.unsqueeze(-2)\n",
    "\n",
    "    #Create eval input table, create buffer\n",
    "    inputs = 2*math.pi*pos_enc*(1/period)**(channel_indices/channel_width)\n",
    "    encodings = torch.zeros([offset + standard_length, channel_width])\n",
    "\n",
    "    #Calculate buffer. Note that I want the offset=0 entry to be a sin instance. I jump through\n",
    "    #a few extra hoops to ensure this.\n",
    "\n",
    "    if offset % 2 == 0:\n",
    "      encodings[::2, :] = torch.sin(inputs[::2, :])\n",
    "      encodings[1::2, :] = torch.cos(inputs[1::2, :])\n",
    "    else:\n",
    "      encodings[1::2, :] = torch.sin(inputs[1::2, :])\n",
    "      encodings[::2, :] = torch.cos(inputs[::2, :])\n",
    "\n",
    "    #Finally, return\n",
    "\n",
    "    return encodings\n",
    "\n",
    "\n",
    "  def _recalculate_needed(self, length, offset):\n",
    "    if offset > self._offset:\n",
    "      return True\n",
    "    if length > self.length:\n",
    "      return True\n",
    "\n",
    "\n",
    "\n",
    "  def _read_buffer(self, length, offset):\n",
    "    \"\"\"\n",
    "    A method to read out of the buffer. If needed, recalculates the buffer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #Figure out if I need to enlarge my buffer. If so, double dimensions which are too small.\n",
    "\n",
    "    if self._recalculate_needed(length, offset):\n",
    "      print(\"recalculate\")\n",
    "      if offset > self._offset:\n",
    "        self._offset = 2*self._offset\n",
    "      if length > self.length:\n",
    "        self._standard = 2*length\n",
    "      self.encodings = self.calculate_buffer(self._offset, self._standard, self._width, self._period)\n",
    "\n",
    "\n",
    "    #Access buffer. Slice out needed segment. Return it\n",
    "    buffer_shift = self._offset - offset\n",
    "    encoding_slice = self.encodings[buffer_shift:(length + buffer_shift)]\n",
    "    return encoding_slice\n",
    "\n",
    "  def __init__(self, offset=0, pos_length=1000, channel_width=100,  period=10000, dilation=1):\n",
    "    #Warm up torch\n",
    "    super().__init__()\n",
    "\n",
    "    #Store variables.\n",
    "    self._offset = offset\n",
    "    self._standard = pos_length\n",
    "    self._width = channel_width\n",
    "    self._period = period\n",
    "\n",
    "    #Store parameters\n",
    "    self._alpha = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    #Create the buffer\n",
    "    buffer = self.calculate_buffer(offset, pos_length, channel_width, period, suppress_errors=True)\n",
    "    self.register_buffer('encodings', buffer)\n",
    "\n",
    "  def forward(self, x, offset):\n",
    "    \"\"\"\n",
    "    The forward pass. Retrieves needed information from buffer,\n",
    "    and then mixes it where requested\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x = x + torch.sigmoid(self._alpha)*self._read_buffer(x.shape[-2], offset)\n",
    "    return x\n",
    "\n",
    "#Unit testing\n",
    "\n",
    "def TestFunc(tensor, init_offset, eval_offset, init_pos_length, channel_width, period):\n",
    "  print(\"Performing unit test for RelPosEncoding\")\n",
    "\n",
    "\n",
    "  items = (tensor.shape, init_offset, eval_offset, init_pos_length, channel_width, period)\n",
    "  msg = \"Parameters are: tensorshape: %s, init_offset: %s, eval_offset: %s, init_pos_length: %s, channel_width: %s, period: %s\" % items\n",
    "  print(msg)\n",
    "  test_layer = RelPosEncoding(init_offset, init_pos_length, channel_width, period)\n",
    "  outcome = test_layer(tensor, eval_offset)\n",
    "  print(\"Output shape is:\")\n",
    "  print(outcome.shape)\n",
    "  return outcome\n",
    "\n",
    "#Run\n",
    "\n",
    "small_vector = torch.zeros([5, 4])\n",
    "test_vector = torch.zeros([10, 100, 150, 5])\n",
    "\n",
    "#Test offset behavior by manual view\n",
    "\n",
    "print(\"Manual inspection: \\n\")\n",
    "print(TestFunc(small_vector, 0, 0, 100, 4, 100))\n",
    "print(TestFunc(small_vector, 1, 1, 100, 4, 100))\n",
    "print(TestFunc(small_vector, 2, 1, 100, 4, 100))\n",
    "\n",
    "#Test handling of buffer resize\n",
    "\n",
    "print(\"Buffer resize: \\n\")\n",
    "TestFunc(test_vector, 5, 10, 200, 5, 10000)\n",
    "TestFunc(test_vector, 5, 10, 50, 5, 10000)\n",
    "'done'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LocalView\n",
    "\n",
    "LocalView is an extremely important class used to efficiently construct a tensor representing local information between tenser entries, even across differing parameters. It places this information in up to two new dimensions, centered on where the original dimension was.\n",
    "\n",
    "**Output**\n",
    "\n",
    "The class expands a dimension, in place, to include three extra dimensions, collocolally referred to as the head, primary, and local dimension. Head is in front of the initial dimension; local behind\n",
    "\n",
    "For example, a tensor of shape (..., view_dim, ...) would\n",
    "have an output shape analogous to (..., head_dim, primary_dim, local_dim, ...)\n",
    "\n",
    "**Parameters: Prior, Post, dim**\n",
    "\n",
    "The parameters prior and post indicate to the class how many neighbors to include in the slicing behavior. Meanwhile, dim is an integer that, straightforwardly, chooses which dimension to perform a local view on. Knowing only this, one can already extract a reasonably accurate local view of a problem.\n",
    "\n",
    "\n",
    "Using just these methods, one can easily extract a local view of a particular dimension, while changing no defaults. For instance:\n",
    "\n",
    "(..., view_dim, ...) -> (..., 1, view_dim, prior + 1 + post)\n",
    "\n",
    "The last dimension ends up giving first all prior items in order, then the current dimension, then the post dimensions. Notably, any missing sections, as would be seen near the start and end, are padded with zeros. The heading dimensions, since only one dilation is used, is one.\n",
    "\n",
    "**Parameters: Stride, Dilations**\n",
    "\n",
    "Stride and Dilation makes things significantly more complicated.\n",
    "\n",
    "Stride controls the rate of traversal along the middle output dimension. That is, how fast does the center viewpiece move along. An insufficient number of viewing items will result in the edges simply being clipped, so keep this in mind.\n",
    "\n",
    "Dilations, on the other hand, control two items. Number one, it controls the dilation parameter of the local dimension. . This is how many units away from the center focus we jump when performing each lookup. Here, any missing information IS handled by padding.\n",
    "\n",
    "\n",
    "Number two, it controls how many heads are produced. Dilations can be a list of ints, or an int. In the case of a list of ints, each dilation parameter will be evaluated independently, and the results stacked and returned along the head dimension.\n",
    "\n",
    "**Performance**\n",
    "\n",
    "A lot of tricks are used to keep the performance high. First, an underlying buffer is constructed which contains all the padding, and the parts declared in the right order. Then, for each dilation a combination of offset, shape, and striding parameters is taken to get the appropriate view. Finally, all the views are stacked then returned.\n",
    "\n",
    "I expect this to be a significant part of the model, and thus it must be fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Buffer**\n",
    "\n",
    "The first thing LocalView"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buffer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing manual test. Verify sequence is dilating\n",
      "input\n",
      "tensor([[ 0,  1],\n",
      "        [ 2,  3],\n",
      "        [ 4,  5],\n",
      "        [ 6,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 11],\n",
      "        [12, 13],\n",
      "        [14, 15],\n",
      "        [16, 17],\n",
      "        [18, 19]])\n",
      "torch.Size([10, 2, 1])\n",
      "output, (squeezed)\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5]],\n",
      "\n",
      "        [[ 2,  3],\n",
      "         [ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 4,  5],\n",
      "         [ 6,  7],\n",
      "         [ 8,  9]],\n",
      "\n",
      "        [[ 6,  7],\n",
      "         [ 8,  9],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11],\n",
      "         [12, 13]],\n",
      "\n",
      "        [[10, 11],\n",
      "         [12, 13],\n",
      "         [14, 15]],\n",
      "\n",
      "        [[12, 13],\n",
      "         [14, 15],\n",
      "         [16, 17]],\n",
      "\n",
      "        [[14, 15],\n",
      "         [16, 17],\n",
      "         [18, 19]]])\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n",
      "tensor([[2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7]])\n",
      "torch.Size([8, 3, 2])\n",
      "Performing unit test for LocalView\n",
      "Inputs are: tensor: torch.Size([30, 24, 4, 6]), prior: 1, post: 1, stride: 1, dilation: 1, dimension: -1\n",
      "Output shape is:\n",
      "torch.Size([30, 24, 4, 6, 3])\n",
      "Performing unit test for LocalView\n",
      "Inputs are: tensor: torch.Size([30, 24, 4, 6]), prior: 1, post: 2, stride: 1, dilation: 1, dimension: -1\n",
      "Output shape is:\n",
      "torch.Size([30, 24, 4, 6, 4])\n",
      "Performing unit test for LocalView\n",
      "Inputs are: tensor: torch.Size([30, 24, 4, 6]), prior: 3, post: 2, stride: 1, dilation: 1, dimension: -1\n",
      "Output shape is:\n",
      "torch.Size([30, 24, 4, 6, 6])\n",
      "Performing unit test for LocalView\n",
      "Inputs are: tensor: torch.Size([30, 24, 4, 6]), prior: 3, post: 2, stride: 4, dilation: 3, dimension: -1\n",
      "Output shape is:\n",
      "torch.Size([30, 24, 4, 1, 6])\n",
      "Performing unit test for LocalView\n",
      "Inputs are: tensor: torch.Size([30, 24, 4, 6]), prior: 3, post: 2, stride: 4, dilation: 3, dimension: -3\n",
      "Output shape is:\n",
      "torch.Size([30, 6, 6, 4, 6])\n",
      "Performing unit test for LocalView\n",
      "Inputs are: tensor: torch.Size([30, 24, 4, 6]), prior: 3, post: 2, stride: 1, dilation: 1, dimension: -1\n",
      "Output shape is:\n",
      "torch.Size([30, 24, 4, 1, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LocalView(nn.Module):\n",
    "  @staticmethod\n",
    "  def split_three_ways(listobject,index):\n",
    "    \"\"\"\n",
    "\n",
    "    Extracts all elements up to an entry, the entry itself, and all elements\n",
    "    after an entry into three separate tensors\n",
    "\n",
    "    \"\"\"\n",
    "    #Calculate implicit, handle negative indexing\n",
    "    listobject = [*listobject[:]]\n",
    "    total_dimensions = len(listobject)\n",
    "    index = index % total_dimensions\n",
    "    #Calculate split quantities\n",
    "\n",
    "    prior_sliceout = index\n",
    "    post_sliceout = prior_sliceout + 1\n",
    "\n",
    "    prior = listobject[:prior_sliceout]\n",
    "    center = listobject[prior_sliceout:post_sliceout]\n",
    "    post = listobject[post_sliceout:]\n",
    "\n",
    "    return prior, center, post\n",
    "\n",
    "  @staticmethod\n",
    "  def merge_three_ways(lista, listb, listc):\n",
    "    \"\"\"\n",
    "    The reverse of s  plit three ways\n",
    "    \"\"\"\n",
    "    return lista + listb + listc\n",
    "\n",
    "  @staticmethod\n",
    "  def merge_three_ways(lista, listb, listc):\n",
    "    \"\"\"\n",
    "    The reverse of s  plit three ways\n",
    "    \"\"\"\n",
    "    return lista + listb + listc\n",
    "  @classmethod\n",
    "  def functional(cls, tensor, prior, post, stride, dilation, dim, pad=True):\n",
    "    \"\"\"\n",
    "    The functional definition of the LocalView Process.\n",
    "\n",
    "    Expects the last dimension to be the one we wish to view\n",
    "\n",
    "    A stride, then pad, then another stride is performed internally.\n",
    "\n",
    "    The extra local channel is defined as the second to last entry in the\n",
    "    tensor dimensions\n",
    "    \"\"\"\n",
    "    #Turn dim into a list if it is not one already\n",
    "\n",
    "    dim = dim % len(tensor.shape)\n",
    "    \n",
    "    #Calculate the undershoot and overshoot. This will be used to either pad to the appropriate length,\n",
    "    #or change the output size.\n",
    "    \n",
    "    dilation_undershoot = prior*dilation #The number of prior segments, times the factor we are extending each segment\n",
    "    dilation_overshoot = post*dilation #The number of post segments, times the factor by which we are extending each segment\n",
    "\n",
    "    #Create the underlying data buffer. Do this by calculating padding quantities, if padding will be used, or else\n",
    "    #just rearranging where the buffer starts\n",
    "    \n",
    "    if pad:\n",
    "        pad_op = (dilation_undershoot, dilation_overshoot)\n",
    "    else:\n",
    "        pad_op = (0, 0)\n",
    "\n",
    "    buffer_tensor = tensor.swapdims(-1, dim)\n",
    "    buffer_tensor = F.pad(buffer_tensor, pad_op)\n",
    "    buffer_tensor = buffer_tensor.swapdims(dim, -1)\n",
    "\n",
    "    #Preprocess as much as possible. The individual dimension lengths and stride shapes\n",
    "    #will change, but these will not. Also, carefully handle the cases with padding, or no padding\n",
    "\n",
    "\n",
    "    shape_priors, shape_peeloff, shape_post = cls.split_three_ways(buffer_tensor.shape, index=dim)\n",
    "    stride_priors, stride_peeloff, stride_post = cls.split_three_ways(buffer_tensor.stride(), index=dim)\n",
    "\n",
    "    if pad:\n",
    "        shape_peeloff = tensor.shape[dim]\n",
    "    else:\n",
    "        shape_peeloff = tensor.shape[dim] - dilation_undershoot - dilation_overshoot\n",
    "    \n",
    "    stride_peeloff = stride_peeloff[0]\n",
    "\n",
    "    ##\n",
    "\n",
    "    dim_shape_length = shape_peeloff//stride #Integer Floor ensures stride jump stops early enough\n",
    "    local_shape_length = prior + 1 + post #Should be self, plus priors and posts. Mind padding is needed!\n",
    "\n",
    "       \n",
    "\n",
    "    #calculate the shape and stride parameters. Then create and return the tensor\n",
    "\n",
    "    dim_stride_size = stride_peeloff*stride #Increase advancement by stride rate\n",
    "    local_stride_size = stride_peeloff*dilation #Increase dilation buffer by dilation rate.\n",
    "\n",
    "    update_shape = [dim_shape_length, local_shape_length]\n",
    "    update_stride = [dim_stride_size, local_stride_size]\n",
    "\n",
    "    reshape = cls.merge_three_ways(shape_priors, update_shape, shape_post)\n",
    "    restride = cls.merge_three_ways(stride_priors, update_stride, stride_post)\n",
    "\n",
    "    output_tensor = buffer_tensor.as_strided(reshape, restride)\n",
    "\n",
    "\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "  def __init__(self, prior, post, stride=1, dilation=1, dimension=-1, pad=True):\n",
    "    \"\"\"\n",
    "    The initializer for layer access. Unlike the\n",
    "    functional variety, this will happily view and restore channels\n",
    "    other than the last one.\n",
    "    \"\"\"\n",
    "    #Spin up torch\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    #Store parameters\n",
    "\n",
    "    self._prior = prior\n",
    "    self._post = post\n",
    "    self._stride = stride\n",
    "    self._dilations = dilation\n",
    "    self._dim = dimension\n",
    "    self._pad = pad\n",
    "  def forward(self, x):\n",
    "\n",
    "    return self.functional(x, self._prior, self._post, self._stride, self._dilations, self._dim, self._pad)\n",
    "\n",
    "#Create unit testing\n",
    "\n",
    "\n",
    "def TestFunc(tensor, prior, post, stride, dilation, dimension, pad=True):\n",
    "  print(\"Performing unit test for LocalView\")\n",
    "\n",
    "\n",
    "  items = (tensor.shape, prior, post, stride, dilation, dimension)\n",
    "  msg = \"Inputs are: tensor: %s, prior: %s, post: %s, stride: %s, dilation: %s, dimension: %s\" % items\n",
    "  print(msg)\n",
    "  test_layer = LocalView(prior, post, stride,  dilation, dimension, pad)\n",
    "  outcome = test_layer(tensor)\n",
    "  print(\"Output shape is:\")\n",
    "  print(outcome.shape)\n",
    "  return outcome\n",
    "\n",
    "simple_test = torch.arange(20)\n",
    "simple_test = simple_test.view([10,2,1])\n",
    "\n",
    "print(\"Performing manual test. Verify sequence is dilating\")\n",
    "print(\"input\")\n",
    "print(simple_test.squeeze())\n",
    "print(simple_test.shape)\n",
    "test_layer = LocalView(1, 1, 1, 1, -3, False)\n",
    "test_output = test_layer(simple_test)\n",
    "test_output = test_output.squeeze()\n",
    "print(\"output, (squeezed)\")\n",
    "print(test_output)\n",
    "print(test_output[0])\n",
    "print(test_output[1])\n",
    "print(test_output.shape)\n",
    "\n",
    "\n",
    "complex_test = torch.zeros([30, 24, 4, 6])\n",
    "\n",
    "#Test base complex cases\n",
    "TestFunc(complex_test, 1, 1, 1, 1, -1)\n",
    "TestFunc(complex_test, 1, 2, 1, 1, -1)\n",
    "TestFunc(complex_test, 3, 2, 1, 1, -1)\n",
    "TestFunc(complex_test, 3, 2, 4, 3, -1)\n",
    "TestFunc(complex_test, 3, 2, 4, 3, -3)\n",
    "cap = TestFunc(complex_test, 3, 2, 1, 1, -1, False)\n",
    "\n",
    "\"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pad\n",
    "\n",
    "One relatively annoying thing about torch is that the padding behavior of torch is difficult to use on off target dimensions.\n",
    "\n",
    "This is just a little function to make life a little easier, allowing for the padding of a target dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": [
    "def Pad(tensor, dimension, paddings):\n",
    "  tensor = tensor.swapdims(-1, dimension)\n",
    "  tensor = F.pad(tensor, paddings)\n",
    "  tensor = tensor.swapdims(-1, dimension)\n",
    "  return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select_Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Reshape\n",
    "\n",
    "The LinearReshape layer combines a View with a Linear layer in order to permit the seamless reshaping of layers. It has particular utility performing actions such as generating or eliminating heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Linear Reshape\n",
      "Inputs are: shape: torch.Size([30, 20, 15, 5, 3]), input_shape: (15, 5, 3), output_shape: 225\n",
      "output shape is:\n",
      "torch.Size([30, 20, 225])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearReshape(nn.Module):\n",
    "  def __init__(self, input_shape, output_shape):\n",
    "    super().__init__()\n",
    "    \n",
    "    input_size, output_size = np.prod(input_shape), np.prod(output_shape)\n",
    "    self._preprocessing = View(input_shape, input_size)\n",
    "    self._linear = nn.Linear(input_size, output_size)\n",
    "    self._postprocessing = View(output_size, output_shape)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self._preprocessing(x)\n",
    "    x = self._linear(x)\n",
    "    x = self._postprocessing(x)\n",
    "    return x\n",
    "\n",
    "#Unit testing\n",
    "\n",
    "\n",
    "\n",
    "def TestLinearReshape(tensor, input_shape, output_shape):\n",
    "    print(\"Test Linear Reshape\")\n",
    "    items = (tensor.shape, input_shape, output_shape)\n",
    "    print(\"Inputs are: shape: %s, input_shape: %s, output_shape: %s\" % items) \n",
    "    layer = LinearReshape(input_shape, output_shape)\n",
    "    output = layer(tensor)\n",
    "    print(\"output shape is:\")\n",
    "    print(output.shape)\n",
    "    return output\n",
    "\n",
    "tensor = torch.zeros((30, 20, 15, 5, 3))\n",
    "\n",
    "TestLinearReshape(tensor, (15, 5, 3), 225)\n",
    "\"done\"\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "Feedforward is a fairly standard, two layer feedforward process for allowing connectons to occur between channel entries in a transformer, or wherever else this might be needed. It works by performing a higher dimensional projection, then allowing the machine itself to choose where and how to slice. \n",
    "\n",
    "It consists of a linear layer, followed by a relu, followed by another linear layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Transformer contains the basic essencials needed in order to make the transformer process into a reality. The transformer utilized in these projects consist of a query, content input which can be simaltaniously driven for self attention, or separately driven for focus attention. Inside the layer, the query, key, and value head projection are located, attention is performed, and the results returned.\n",
    "\n",
    "Notably, masking is determined by the mask parameter, which can be fed in at initiation time, or run time. If fed during initiation, it must be provided as \"lower\", \"upper\", or \"None\". Runtime can be anything, but must be compatible under matrix multip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing test of Transformer\n",
      "Inputs are: shape: torch.Size([10, 30, 40, 10]), shape: torch.Size([10, 30, 20, 10]) channel_width: 10, head_width: 10, mask_type: None, mask_value: None\n",
      "output shape is: torch.Size([10, 30, 40, 10])\n",
      "Performing test of Transformer\n",
      "Inputs are: shape: torch.Size([10, 30, 40, 10]), shape: torch.Size([10, 30, 20, 10]) channel_width: 10, head_width: 20, mask_type: None, mask_value: None\n",
      "output shape is: torch.Size([10, 30, 40, 10])\n",
      "Performing test of Transformer\n",
      "Inputs are: shape: torch.Size([10, 30, 40, 10]), shape: torch.Size([10, 30, 20, 10]) channel_width: 10, head_width: 10, mask_type: lower, mask_value: None\n",
      "output shape is: torch.Size([10, 30, 40, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    __permitted = (None, \"lower\", \"upper\")\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "    @mask.setter\n",
    "    def mask(self, value):\n",
    "        assert value in self.__permitted, \"mask cannot be set to this\"\n",
    "        self._mask = value\n",
    "    def __init__(self, channel_dim, head_width, mask=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Accepted mask is \"lower\", \"upper\", or none\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #Spin up torch\n",
    "        super().__init__()\n",
    "        \n",
    "        #Create action generators\n",
    "        QueryGen = LinearReshape(channel_dim, (head_width, channel_dim))\n",
    "        KeyGen = LinearReshape(channel_dim, (head_width, channel_dim))\n",
    "        ValGen = LinearReshape(channel_dim, (head_width, channel_dim))\n",
    "        \n",
    "        CollapseGen = LinearReshape((head_width, channel_dim), channel_dim)\n",
    "        \n",
    "        #Create actions. Note the swap is needed to get the head in front of the items. \n",
    "        \n",
    "        self._query = lambda x : QueryGen(x).swapdims(-2, -3) \n",
    "        self._key = lambda x : KeyGen(x).swapdims(-2, -3)\n",
    "        self._value = lambda x : ValGen(x).swapdims(-2, -3)\n",
    "        self._dehead = lambda x : CollapseGen(x.transpose(-2, -3))\n",
    "        \n",
    "        self.mask = mask\n",
    "        \n",
    "    def forward(self, query, content, mask=None):\n",
    "        #Create query, key, value\n",
    "        \n",
    "        query = self._query(query)\n",
    "        key = self._key(content).swapdims(-1, -2)\n",
    "        value = self._value(content)\n",
    "        \n",
    "        #Create focus matrix. Mask. Softmax.\n",
    "        \n",
    "        focus = query.matmul(key)\n",
    "        focus_dims = focus.shape[-2:]\n",
    "        if mask is None:\n",
    "            #Runs only if not provided a mask.\n",
    "            if self.mask == \"lower\":\n",
    "                mask = torch.tril(torch.ones(focus_dims))\n",
    "                focus = focus.masked_fill(mask == 0, -1e9)\n",
    "            if self.mask == \"upper\":\n",
    "                mask = torch.triu(torch.ones(focus_dims))\n",
    "                focus = focus.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        focus = F.softmax(focus, dim=-1)\n",
    "        \n",
    "        #Apply focus matrix to values. Then compact head\n",
    "        \n",
    "        output = focus.matmul(value)\n",
    "        output = self._dehead(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "#Unit code.\n",
    "\n",
    "def TestFunc(query, content, channel_width, head_width, mask_type, mask_value = None):\n",
    "    print(\"Performing test of Transformer\")\n",
    "    items = (query.shape, content.shape, channel_width, head_width, mask_type, mask_value)\n",
    "    print(\"Inputs are: shape: %s, shape: %s channel_width: %s, head_width: %s, mask_type: %s, mask_value: %s\" % items)\n",
    "    test_layer = Transformer(channel_width, head_width, mask_type)\n",
    "    test_output = test_layer(query, content, mask_value)\n",
    "    print(\"output shape is: %s\"  % str(test_output.shape))\n",
    "    return test_output\n",
    "\n",
    "test_query = torch.zeros(10, 30, 40, 10)\n",
    "\n",
    "item = torch.zeros(10, 30, 40, 10, 10)\n",
    "item.view((10, 30, 40, 100))\n",
    "test_content = torch.zeros(10, 30, 20, 10)\n",
    "TestFunc(test_query, test_content, 10, 10, None)\n",
    "TestFunc(test_query, test_content, 10, 20, None)\n",
    "TestFunc(test_query, test_content, 10, 10, \"lower\")\n",
    "\n",
    "\"done\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakthrough\n",
    "\n",
    "Extremely potent algorithms which will be used for many purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback Transformer\n",
    "\n",
    "Transformers are good when you want it to be the case that the items of the output should be equal to the query. Unfortunately, they are not quite as great when the items in the output should be equal to the items in the content. The feedback transformer is my solution to this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize Transformer\n",
    "\n",
    "Transformers will be utilized to resize the layers, but that still leaves a problem: Generating the queue. While it will be the case that the Key and Value can be generated from the incoming stream directly, the Queue needs to be the length of the \n",
    "output to work correctly. \n",
    "\n",
    "\n",
    "\n",
    "**Queue**\n",
    "\n",
    "The queue is generated by performing an interleave expansion, a view, and then either a max or a mean. \n",
    "\n",
    "Let K be the initial embeddings size, and L be the target embeddings size. Perform a repeat interleave along embeddings of length L. The embeddings now cleanly divide into L. Go and perform a view on the embeddings, looking at (L, K). It is now the case that one dimension is the right shape. \n",
    "\n",
    "At this point, any summarizing statistic can be performed on the last embedding dimension to get something of length L. Mean is predicted to be particularly helpful when operating locally, but max might be useful as well.\n",
    "\n",
    "**Transformer**\n",
    "\n",
    "With a Queue tensor made, Transfomer can be executed. Take the Queue, take the Content, expand heads and transform. Resize is then complete. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Transformers\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "Transformers are quite effective, however they suffer from some flaws, particularly when working with extremely large text corpuses. The primary problem is that though one might have more than enough computational capacity, the memory interchange becomes burdensome as the input widths continue to increase. This is by virtue of the fact that every input must condition every output in order for the transformer process to work properly.\n",
    "\n",
    "The Local Philosophy is a way around this. It consists of creating locally relevant views of the incoming data, and performing transformer on that. In this way, the same computation can be performed with relatively little intercommunication. \n",
    "\n",
    "Meanwhile, a global feedback mechanism ensures that global feedback is generated in several stages, and fed back into the model. This global feedback mechanism is meant to contain far fewer channels of information than a standard transformer. \n",
    "\n",
    "Both an ability to resize, along with an ability to consider things locally, is needed for this to work. Thus the development of the local resize transformer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Resize\n",
    "\n",
    "Local Resize is more or less just a mechanism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Resize Transformer\n",
    "\n",
    "The Local Resize Transformer consists of a transfomer capable of elegantly performing local resize operations. It consists of two primary features. These are:\n",
    "\n",
    "* Resize Generation\n",
    "* Query, Content operation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Resize Transfomer\n",
    "\n",
    "Local transfomer consists of a set of methods for local transformer, with possible resize, to occur\n",
    "\n",
    "**Local Transformer**\n",
    "\n",
    "Local transformer is a variety of transformer developed for two purposes. One of these is to take advantage of local information, such as the relative positioning of words. The other is to enable incredibly parallel computations.\n",
    "\n",
    "Overlapping views are created of an input \n",
    "\n",
    ". One problem which traditional transformers are faced with is that \n",
    "\n",
    "optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisive Resize  Gradient\n",
    "\n",
    "The Decisive Virtual Gradient is a philosophy and set of classes and methods designed to allow the smooth recovery of a differential gradient in a situation which might otherwise not permit it. It is designed particularly to function in discard situations - where information will be permanently lost. This scenario is not an altogether uncommon situation, appearing everywhere in life. Despite this, there are few tools to smoothly handle the situation. As a result, I developed the Decisive virtual gradient. \n",
    "\n",
    "The conditions required for this algorithm to function are related to the loss, which must be labeled, and to the type of decision, which must be one of sorting, where ensuring the proper number of items are sorted into a particular catagory is what matters. \n",
    "### Explanation\n",
    "\n",
    "\n",
    "Let there be some circumstance in an algorithm in which one wants to make a non-differentiable decision affecting the configuration of the rest of the model. This might occur if, for example, you are deciding whether to expand the number of words in a string, or discard information down a relu.\n",
    "\n",
    "Let it be the case that there are a clear set of decision labels y_d capable of telling us what the proper configuration at the end should be, in either a statistical or direct manner. Let there be a hidden layer, H, with input parameters h, at which we need to make a decision. Let it be the case that the decision is made by taking h and using it to, for each batch entry, form a set of logits representing each entry, with the argmax indicating the actual decision. \n",
    "This is difficult to handle using standard methods. Although policy gradients can help select proper parameters, they are exaustive and slow. Standard gradient descent will fail utterly without a manifold. Instead, we do the following.\n",
    "\n",
    "#### Virtual Gradient Resize\n",
    "\n",
    "The decision process is designed to produce, at the end, a \"handle\" of sorts in the neural network through which, by knowing label information regarding required tensor length later on, we can expand or contract at will while utilizing gradient descent. The process works something like follows. \n",
    "\n",
    "There exists any number of standard tensors, and a special tensor, known as history. There also exists a layer, known as decisive resize, which performs the following action. Channels are reduced down to a sequence of logits by a linear layer, and from this sequence whether to pass this forward or not into the corrosponding output tensor is determined by whether the logit is positive or negative. Then, the logits are split into two, by their positive and negative sections, and \n",
    "\n",
    "\n",
    "\n",
    "Once this decision is made, the logit tensors are split into positve and negative versions, then abs, summed, and normalized, then concatenated together. This tensor is then known as local history. Local history is then added to the incoming global history tensor, producing a new global history. The output tensor is then fed forward, along with the global history, until the loss endpoint is reached\n",
    "\n",
    "#### Loss endpoint\n",
    "\n",
    "The loss endpoint is fed two items. First, there are the tensors about which we actually want to make a decision on. Second, there is the history tensor, a backpropagation handle. The endpoint first evaluates the shape of the output - primarily whether it is acceptable. If it is not, it uses the handle to produce a loss and indicate this. Else, the\n",
    "\n",
    "Local history has the property that if you wanted to shut off some of the individual logits, one could penalize the true logit portion and incentivise the false logit portion. Vise Versa is also true. This allows the control, on a local level, of parameters which do not have nice gradients. \n",
    "\n",
    "Further control may then be gained by \n",
    "\n",
    "\n",
    "* Take in History, Tensor.\n",
    "* use tensor. Create binary decision,\n",
    "* Separate logits by true, false. Sum results\n",
    "* Normalize logits by number of true/false cases\n",
    "* Add to history. \n",
    "\n",
    "\n",
    "#### Decision \n",
    "\n",
    "We start by making a decision kernel. For each individual decision, and each incoming channel of whatever nature, we run it through a linear which outputs options according to the number of decisions which are available. We then split it into two sections, based on the sign of the logits. If the sign is positive, the case is said to be accepted, and will be passed to that output. If negative, it is said to be negative, and will not be passed forward out that channel\n",
    "\n",
    "Internal decision order is preserved.\n",
    "\n",
    "#### Decision Graph Backpropagation.\n",
    "\n",
    "This is, of course, impossible to perform gradient descent against, so I introduce decision graph backpropogation. The idea behind this technique is to turn the logits into probabilities, update these as we go along, and use them  . First, we split the logits up into two tensors, containing only the positive\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Decision\n",
    "\n",
    "We begin by making a decision kernel, which for each individual decision option runs it through a linear to form a logits representing the choices and their weights. This is then utilized for a maxarg based decision, telling us where to place each input. The inputs are then routed to the appropriate output, and returned\n",
    "\n",
    "Meanwhile, back at the logits point, an additional action occurs. An input into this layer should consist of any previous \n",
    "routing action. Once the logits are developed, each logit is sorted into two catagories, consisting of the null representitives and active representitives\n",
    "At each decision point, we begin by defining a decision kernel, and decision space, consisting of taking the input \n",
    "\n",
    "\n",
    "\n",
    "We construct a graph using a sort of \"routing\" logic fairly similar to \n",
    "\n",
    "#### Forward\n",
    "\n",
    "\n",
    "Accept in the values of h. Out of these, make the decisive logits. Using the decisive logits, perform the decision. Then we cache the decisive logits.\n",
    "\n",
    "Then it is time to create the representation. The represention is created by summing up the logits per catagory, and is then conditioned by hinge loss later.\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "#### Loss\n",
    "\n",
    "Loss is calculated in one of two ways. It is first calculated according to decision parameters. If this is passed, it is then calculated in terms of standard gradient descent. Due to labels frequently being of a particular size, it was felt this is needed. \n",
    "\n",
    "When loss is calculating in decision mode, the gradient parameters are the maximum and minimum counts for particular categories. The layers then must know what to do with this.\n",
    "\n",
    "==\n",
    "\n",
    "#### Backwards\n",
    "\n",
    "During back propogation, additional tensor information is \n",
    "\n",
    "\n",
    "\n",
    "The decisive virtual gradient consists of \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let it be the case that the actual decision is made by turning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
       "        30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
       "        30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
       "        30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
       "        30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
       "        30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
       "        30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
       "        30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
       "        30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0001,\n",
       "        30.0000, 30.0000, 29.9999, 30.0000, 30.0000, 30.0000, 30.0000, 29.9999,\n",
       "        29.9999, 30.0000, 30.0000, 30.0000, 30.0000, 29.9999, 29.9999, 30.0000,\n",
       "        30.0001, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0001, 30.0001,\n",
       "        30.0000, 30.0000, 30.0000, 30.0001], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.arange(3000).view(100, 30).type(torch.float32)\n",
    "layer = nn.LayerNorm(30)\n",
    "output = layer(test)\n",
    "torch.square(output).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyramid\n",
    "\n",
    "It is the case that many of the models I am trying to develop follow a particular pattern. This pattern is that they are located orthogonally, with escalation, then higher order processing, then finally lower order processing occuring. It is the case that operating according to this pattern consistently without some framework will prove quite difficult. \n",
    "\n",
    "### Motivation\n",
    "\n",
    "It is the case that the further you get from the input source, the harder a model gets to train. Gradient descent tends to blow up or decay to nothing, or even just have trouble getting started, if an excessively deep model is used. Nonetheless, it is the case that deep models are needed to handle complex behavior, but needs the shallow parts to handle the simple cases. \n",
    "\n",
    "This begs the question: What would be an ideal model construction? One might say that an ideal model might handle simple and shallow behavior with simple but extensive lower layers, and use higher but more compact layers to represent global state. Additionally, an ideal model might not force the entirety of the model to be used at all for more simple cases.\n",
    "\n",
    "One method of constructing something such as this might be the escalating pyramid seen in (item). This is, without a doubt, functional. It is excellent in cases with few priors about the arrangement of information.  However, it is the case this may be significantly suboptimal for the case where local relations are present between the items. Additional problems also present themselves: frequently, it is the case that the big picture will subsequently inform the way the little details needed to be examined, and such a model does not efficiently take this into account, instead forcing such analysis on the hgiher levels of the model. Ideally, we would instead like tbhe model itself to possess the ability to condition the output using it's higher level items, in a somewhat recursive process.\n",
    "\n",
    "It also is scarcely designed for transformer networks. An additional framework is needed, one which can deal with the local, while being informed by the global.\n",
    "\n",
    "This section provides such framework. The class interface consists of three parts, and an init. These are the Processing, Integration, and Finally methods.\n",
    "### Interface\n",
    "\n",
    "**Init**\n",
    "\n",
    "Init allows the definition of the model as an terminal or intermediary layer. It also controls whether feedback\n",
    "is included per layer or not.\n",
    "\n",
    "**Processing**\n",
    "\n",
    "condition will accept the input and any feedback; in the case of no feedback, the feedback slot will be provided with None. It will develop two outputs which are the conditioning and processing tensor respectively. The processing tensor will be escalated deeper into the pyramid, while the conditioning tensor will be fed forward into the feedback layer\n",
    "\n",
    "**Integration**\n",
    "\n",
    "The integration layer is responsible for making the results more local. The integration layer is fed the conditioning tensor and the results of processing. It may then return either a single results tensor, or a results tensor and a feedback tensor. In the event of being fed a feedback tensor, this tensor will then be utilized to \n",
    "\n",
    "**Finally**\n",
    "\n",
    "The finally method is fed the initial input and the integration output. It primarily exists for the purpose of normalization tricks. It should return a single output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidMixIn():\n",
    "    \"\"\"\n",
    "    \n",
    "    This is a class for mixing in pyramid\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, terminal=False):\n",
    "        super().__init__() #Start up whatever environment we are in\n",
    "        self.__terminal = terminal\n",
    "        self.__made = False\n",
    "        self.__graph = None\n",
    "        \n",
    "    #Abstract methods.\n",
    "    def Processing(self, stream, feedback):\n",
    "        \"\"\"\n",
    "        \n",
    "        return: conditioning, processing\n",
    "        \n",
    "        \"\"\"\n",
    "        raise NotImplimentedError(\"processing must be implemented\")\n",
    "    def Primary(self, stream, feedback):\n",
    "        raise NotImplimentedError(\"Primary must be implemented\")\n",
    "    def Integration(self, stream, conditioning):\n",
    "        raise NotImplementedError(\"integration must be implemented\")\n",
    "    def Finally(self, input_stream, output_stream):\n",
    "        raise NotImplementedError(\"Finally must be implemented\")\n",
    "        \n",
    "        \n",
    "    #Logical method. Contains the actual logic for the class.\n",
    "    def logical(self, superior, stream, status):\n",
    "        \"\"\"\n",
    "        \n",
    "        The primary logic for the class is located here. \n",
    "        \n",
    "        First, feedback is defined if not already in existance. Then\n",
    "        \"\"\"\n",
    "\n",
    "        #Get the feedback. If there is not enough, set it to none.\n",
    "        if len(status) > 0:\n",
    "            feedback = status.pop()\n",
    "        else:\n",
    "            feedback = None\n",
    "\n",
    "        #Perform processing. Catch any errors\n",
    "        try:\n",
    "            conditioning, processing = self.Processing(stream, feedback)\n",
    "        except Exception as err:\n",
    "            msg = \"Problem running Processing:\" + err\n",
    "            raise Exception(msg) from err\n",
    "\n",
    "        #Perform escalation or primary. Catch error\n",
    "        try:\n",
    "            if superior is None:\n",
    "                if len(status) > 0:\n",
    "                    feedback = status.pop()\n",
    "                else:\n",
    "                    feedback = None\n",
    "                    \n",
    "                items  = self.Primary(processing, feedback)\n",
    "                #Take apart, form initial feedback\n",
    "                if isinstance(items, tuple):\n",
    "                    commands, feedback = items\n",
    "                else:\n",
    "                    commands = items\n",
    "                    feedback = None\n",
    "                \n",
    "                #Start status.\n",
    "                \n",
    "                status = [feedback] #Starts here, at the terminal layer.\n",
    "            else:\n",
    "                commands, status = superior(processing)\n",
    "        except Exception as err:\n",
    "            msg = \"Problem running escalation: \" + error\n",
    "            raise Exception(msg) from err\n",
    "\n",
    "        #Perform integration, catch error, attach feedback.\n",
    "        try:\n",
    "            integration = self.Integration(commands, conditioning)\n",
    "            \n",
    "            if isinstance(integration, tuple):\n",
    "                integration, status = integration\n",
    "                status.append(item)\n",
    "            else:\n",
    "                status.append(None)\n",
    "                \n",
    "        except Exception as err:\n",
    "            msg = \"Problem running Integration: \"  + err\n",
    "            raise Exception(msg) from err\n",
    "\n",
    "        #Perform finally. Catch error\n",
    "\n",
    "        try:\n",
    "            final = self.Finally(stream, integration)\n",
    "        except Exception as err:\n",
    "            msg = \"Problem runnning Finally: \" + err\n",
    "            raise Exception(msg) from err\n",
    "        \n",
    "        #Finished. Return\n",
    "\n",
    "        return final, status\n",
    "    def construct(self, superior):\n",
    "        def graph(stream, status):\n",
    "            self.logical(superior, stream, status)\n",
    "        return graph\n",
    "            \n",
    "    def run(self, item):\n",
    "        \"\"\"\n",
    "        \n",
    "        The run function contains the construction logic and application logic for the problem.\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        if not isinstance(item, (tuple, list)) and not callable(item):\n",
    "            item = [item, []]        \n",
    "\n",
    "        \n",
    "        #Handle the construction logic.\n",
    "        if self.__made == False: ## If I still need to construct my graph.\n",
    "            if not callable(item):\n",
    "                 \n",
    "                item = lambda func : func(item) # Will return the final construction.\n",
    "            \n",
    "            \n",
    "            if self.__terminal is True:\n",
    "                #It is the case this is the terminal layer. Go and run all the factories. Then store, on last layer.\n",
    "                self.__made = True\n",
    "                print(\"bere\")\n",
    "                self.__graph = item(self.construct(None))\n",
    "                print(\"there\")\n",
    "            else:\n",
    "                #This is not the terminal. Make a factory.\n",
    "                def factory(superior):\n",
    "                    #I just got told what my superior's constructor is. \n",
    "                    self.__made = True #I will now know  I have already had my graph constructed.\n",
    "                    return item(self.construct(superior)) #Construct it, then feed it up the chain.\n",
    "                return factory\n",
    "        \n",
    "        elif self.__graph is not None:\n",
    "            return self.__graph(*item) #At final layer. Execute graph\n",
    "        else:\n",
    "            return item #Wait until final layer to execute.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n",
      "fum\n",
      "test\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "class Foo():\n",
    "    def __init__(self):\n",
    "        print(\"foo\")\n",
    "        super().__init__()\n",
    "\n",
    "class Fum():\n",
    "    def __init__(self):\n",
    "        print(\"fum\")\n",
    "    def test(self):\n",
    "        print(\"fum2\")\n",
    "        \n",
    "class Fee(Foo, Fum):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "Fee()\n",
    "        \n",
    "print(\"test\")\n",
    "\n",
    "print(callable(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}