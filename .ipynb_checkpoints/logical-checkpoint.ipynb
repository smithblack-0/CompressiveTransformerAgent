{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LocalView' from 'Base' (C:\\Users\\chris\\Documents\\CompressiveTransformerAgent\\Base\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20736/1963854169.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mBase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mBase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLocalView\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LocalView' from 'Base' (C:\\Users\\chris\\Documents\\CompressiveTransformerAgent\\Base\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from Base import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Subordinate\n",
    "\n",
    "A subordinates job consists of three primary items:\n",
    "\n",
    "* 1: A subordinate should accept a compression stream and any of it's cached feedback then utilize this to compress the stream.\n",
    "* 2: A subordinate should accept an expansion stream and any cached feedback and use this to generate compression feedback and an lower level command stream\n",
    "* 3: Purge it's cache as instances finish evaluating\n",
    "\n",
    "It should also be noted that:\n",
    "\n",
    "* Subordinates contain two heads. The fetch head and internal heads. It is important to note that parameters for the global head should be independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Subordinate():\n",
    "\n",
    "    \"\"\" The subordinate.\n",
    "\n",
    "    Possesses an initialization method, a summarize method, and a feedback method.\n",
    "\n",
    "    This is responsible for escalating to the executive, and retrieving information later on.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_width, encoding_length):\n",
    "        self.width = channel_width\n",
    "        self.cache_depth = encoding_length\n",
    "\n",
    "        self.lookup_cache = None\n",
    "        self.feedback_cache = None\n",
    "        self.command_cache = None\n",
    "\n",
    "    def summarize(self, stream):\n",
    "        \"\"\"\n",
    "\n",
    "        The summarize method.\n",
    "\n",
    "        This is responsible for reducing the number of blocks and thus\n",
    "        performing fetches. It accepts something of shape\n",
    "\n",
    "        (batch_size, iteration, fetch_head, ..., next_block, this_block, channels)\n",
    "\n",
    "        and turns it into something of shape\n",
    "\n",
    "        (iteration, batch_size, fetch_head, ..., next_block, this_block, channels)\n",
    "\n",
    "        while storing a feedback construction cache. It is also responsible for forming\n",
    "        the lookup cache on starting a new iteration.\n",
    "        :return:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #Handle getting started. Make the zeros padding templates I\n",
    "\n",
    "        if self.lookup_cache == None:\n",
    "            #create the inserts if not yet made\n",
    "            shape = list(stream[0].shape)\n",
    "            shape.pop(-2)\n",
    "\n",
    "\n",
    "            self.lookup_cache =  stream[0].unsqueeze(0)\n",
    "            self.feedback_cache = torch.zeros_like(stream[0].unsqueeze(0))\n",
    "            self.command_cache = torch.zeros(shape)\n",
    "\n",
    "        else: #insert the new components, pop off old ones\n",
    "\n",
    "            self.lookup_cache = torch.concat([stream[0].unsqueeze(0), self.lookup_cache], dim=0)\n",
    "\n",
    "            self.feedback_cache = F.pad(self.)\n",
    "        #Insert\n",
    "\n",
    "        #Insert the new lookup cache, used to generate the feedback.\n",
    "        #Move everything down by one\n",
    "        #Pop off used up cache entries.\n",
    "\n",
    "        if self.lookup_cache == None #handles startup\n",
    "            self.lookup_cache = stream[0]\n",
    "        else:\n",
    "            self.lookup_cache = torch.concat([stream[0], self.lookup_cache], dim=0)\n",
    "\n",
    "\n",
    "            self.feedback_cache = torch.concat([torch.zeros_like(stream[0]), self.feedback_cache], dim=0)\n",
    "\n",
    "        if self.lookup_cache.shape[0] > self.cache_depth:\n",
    "            self.lookup_cache = self.lookup_cache[:self.cache_depth]\n",
    "            self.feedback_cache = self.feedback_cache[:self.cache_depth]\n",
    "\n",
    "        #Apply transformer layers.\n",
    "\n",
    "        #Apply reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The preprocessing structure is meant to perform tasks such as embedding, padding to an appropriate width, and such for the model.\n",
    "\n",
    "* The preprocessing layer should embed, pad, and block transform the input.\n",
    "* It is responsible for caching inputs for however wide the output might be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive\n",
    "\n",
    "The executive sits at the top of the chain of command, and is responsible for making decisions, summarizing the situation, and issuing orders. The executive contains a cache of it's own internal history, and is where destination information will ultimately be recorded and read from. Importantly, the executive also has the responsibility of taking in the escalated summary, storing and managing it, then providing directive feedbacks to the subordinates. It is also where the fetch head is generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
